2023-10-03 13:08:34,430 [ERROR] trainer4/trainer4.py Failed to reconnect: Already connected
Traceback (most recent call last):
  File "/Users/miguellopes/Documents/pod-trader/trainer4/trainer4.py", line 102, in disconnect
    sio.connect('http://localhost:5678')
  File "/Users/miguellopes/miniforge3/envs/trader/lib/python3.10/site-packages/socketio/client.py", line 306, in connect
    raise exceptions.ConnectionError('Already connected')
socketio.exceptions.ConnectionError: Already connected
2023-10-03 13:08:34,644 [WARNING] Namespace error occurred: / is not a connected namespace.
2023-10-03 13:10:01,955 [ERROR] packet queue is empty, aborting
2023-10-03 16:31:55,736 [ERROR] trainer4/trainer4.py Failed to reconnect: Already connected
Traceback (most recent call last):
  File "/Users/miguellopes/Documents/pod-trader/trainer4/trainer4.py", line 102, in disconnect
    sio.connect('http://localhost:5678')
  File "/Users/miguellopes/miniforge3/envs/trader/lib/python3.10/site-packages/socketio/client.py", line 306, in connect
    raise exceptions.ConnectionError('Already connected')
socketio.exceptions.ConnectionError: Already connected
2023-10-03 17:16:51,986 [ERROR] packet queue is empty, aborting
2023-10-03 17:22:58,718 [INFO] 
| `step`: Number of samples, or total training steps, or running times of `env.step()`.
| `time`: Time spent from the start of training to this moment.
| `avgR`: Average value of cumulative rewards, which is the sum of rewards in an episode.
| `stdR`: Standard dev of cumulative rewards, which is the sum of rewards in an episode.
| `avgS`: Average of steps in an episode.
| `objC`: Objective of Critic network. Or call it loss function of critic network.
| `objA`: Objective of Actor network. It is the average Q value of the critic network.
|     step      time  |     avgR    stdR    avgS  |     objC      objA
2023-10-03 17:25:39,080 [INFO] | 2.00e+04       160  |    44.49    0.73  300000  |     0.26      0.03
2023-10-03 17:28:20,137 [INFO] | 4.00e+04       321  |    43.65    1.01  300000  |     0.41      0.03
2023-10-03 17:31:05,705 [INFO] | 6.00e+04       487  |    43.92    1.17  300000  |     0.53      0.01
2023-10-03 17:33:46,400 [INFO] | 8.00e+04       648  |    43.55    1.42  300000  |     0.29      0.00
2023-10-03 17:36:27,044 [INFO] | 1.00e+05       808  |    44.01    1.20  300000  |     0.52      0.01
2023-10-03 17:38:54,179 [INFO] 
| `step`: Number of samples, or total training steps, or running times of `env.step()`.
| `time`: Time spent from the start of training to this moment.
| `avgR`: Average value of cumulative rewards, which is the sum of rewards in an episode.
| `stdR`: Standard dev of cumulative rewards, which is the sum of rewards in an episode.
| `avgS`: Average of steps in an episode.
| `objC`: Objective of Critic network. Or call it loss function of critic network.
| `objA`: Objective of Actor network. It is the average Q value of the critic network.
|     step      time  |     avgR    stdR    avgS  |     objC      objA
2023-10-03 17:42:27,444 [INFO] | 2.00e+04       213  |    43.63    1.34  300000  |     0.29      0.01
2023-10-05 10:31:46,338 [INFO] 
| `step`: Number of samples, or total training steps, or running times of `env.step()`.
| `time`: Time spent from the start of training to this moment.
| `avgR`: Average value of cumulative rewards, which is the sum of rewards in an episode.
| `stdR`: Standard dev of cumulative rewards, which is the sum of rewards in an episode.
| `avgS`: Average of steps in an episode.
| `objC`: Objective of Critic network. Or call it loss function of critic network.
| `objA`: Objective of Actor network. It is the average Q value of the critic network.
|     step      time  |     avgR    stdR    avgS  |     objC      objA
2023-10-05 10:38:01,748 [INFO] | 2.00e+04       375  |     9.68   11.22  300000  |     0.01      0.43
2023-10-05 10:44:13,708 [INFO] | 4.00e+04       747  |    10.65    4.97  300000  |     0.00      0.45
2023-10-05 10:50:20,050 [INFO] | 6.00e+04      1114  |     2.03    3.21  300000  |     0.00      0.43
2023-10-05 10:56:26,534 [INFO] | 8.00e+04      1480  |     7.26    3.53  300000  |     0.00      0.43
2023-10-05 11:02:28,784 [INFO] | 1.00e+05      1842  |    13.04    6.47  300000  |     0.00      0.43
2023-10-05 11:11:10,788 [INFO] price_array: 72540
2023-10-05 11:11:10,790 [INFO] | load actor from: ./trained_models/trainer4-87cd7-steps-300000[256, 128]-2019-01-01/actor.pth
2023-10-05 11:11:21,046 [INFO] Test Finished!
2023-10-05 11:11:21,047 [INFO] episode_return: 1.260114880169666
2023-10-05 13:23:15,254 [INFO] 
| `step`: Number of samples, or total training steps, or running times of `env.step()`.
| `time`: Time spent from the start of training to this moment.
| `avgR`: Average value of cumulative rewards, which is the sum of rewards in an episode.
| `stdR`: Standard dev of cumulative rewards, which is the sum of rewards in an episode.
| `avgS`: Average of steps in an episode.
| `objC`: Objective of Critic network. Or call it loss function of critic network.
| `objA`: Objective of Actor network. It is the average Q value of the critic network.
|     step      time  |     avgR    stdR    avgS  |     objC      objA
2023-10-05 13:29:39,506 [INFO] | 2.00e+04       384  |     7.80    7.41  300000  |     0.00      0.44
2023-10-05 13:35:57,014 [INFO] | 4.00e+04       762  |     4.46    8.08  300000  |     0.00      0.43
2023-10-05 13:42:18,887 [INFO] | 6.00e+04      1144  |    -0.51    7.46  300000  |     0.00      0.42
2023-10-05 13:48:26,875 [INFO] | 8.00e+04      1512  |     5.76    6.36  300000  |     0.00      0.46
2023-10-05 13:54:29,281 [INFO] | 1.00e+05      1874  |    -0.15    7.86  300000  |     0.00      0.42
2023-10-05 14:02:55,798 [INFO] price_array: 72540
2023-10-05 14:02:55,800 [INFO] | load actor from: ./trained_models/trainer4-a9ac7-steps-300000[256, 128]-2019-01-01/actor.pth
2023-10-05 14:03:06,803 [INFO] Test Finished!
2023-10-05 14:03:06,803 [INFO] episode_return: 1.7773084867574533
2023-10-05 14:59:44,778 [INFO] 
| `step`: Number of samples, or total training steps, or running times of `env.step()`.
| `time`: Time spent from the start of training to this moment.
| `avgR`: Average value of cumulative rewards, which is the sum of rewards in an episode.
| `stdR`: Standard dev of cumulative rewards, which is the sum of rewards in an episode.
| `avgS`: Average of steps in an episode.
| `objC`: Objective of Critic network. Or call it loss function of critic network.
| `objA`: Objective of Actor network. It is the average Q value of the critic network.
|     step      time  |     avgR    stdR    avgS  |     objC      objA
2023-10-05 15:01:52,508 [INFO] | 2.00e+04       128  |     0.22    0.23  100000  |     0.00      0.43
2023-10-05 15:04:00,307 [INFO] | 4.00e+04       256  |     0.08    0.18  100000  |     0.00      0.44
2023-10-05 15:06:09,468 [INFO] | 6.00e+04       385  |     0.10    0.18  100000  |     0.00      0.42
2023-10-05 15:08:15,340 [INFO] | 8.00e+04       511  |     1.13    1.22  100000  |     0.00      0.42
2023-10-05 15:10:09,691 [INFO] | 1.00e+05       625  |    -0.23    0.10  100000  |     0.00      0.43
2023-10-05 15:19:09,591 [INFO] price_array: 72540
2023-10-05 15:19:09,594 [INFO] | load actor from: ./trained_models/trainer4-65b8e-steps-100000[256, 128, 64]-2019-01-01/actor.pth
2023-10-05 15:19:19,438 [INFO] Test Finished!
2023-10-05 15:19:19,440 [INFO] episode_return: 1.0948276711863043
2023-10-05 16:10:32,571 [INFO] 
| `step`: Number of samples, or total training steps, or running times of `env.step()`.
| `time`: Time spent from the start of training to this moment.
| `avgR`: Average value of cumulative rewards, which is the sum of rewards in an episode.
| `stdR`: Standard dev of cumulative rewards, which is the sum of rewards in an episode.
| `avgS`: Average of steps in an episode.
| `objC`: Objective of Critic network. Or call it loss function of critic network.
| `objA`: Objective of Actor network. It is the average Q value of the critic network.
|     step      time  |     avgR    stdR    avgS  |     objC      objA
2023-10-05 16:12:54,841 [INFO] | 2.00e+04       142  |    -0.09    0.05  100000  |     0.00      0.44
2023-10-05 16:15:16,719 [INFO] | 4.00e+04       284  |    -0.09    0.03  100000  |     0.00      0.44
2023-10-05 16:17:40,298 [INFO] | 6.00e+04       428  |    -0.09    0.04  100000  |     0.00      0.42
2023-10-05 16:19:54,610 [INFO] | 8.00e+04       562  |    -0.12    0.04  100000  |     0.00      0.42
2023-10-05 16:22:04,938 [INFO] | 1.00e+05       692  |    -0.14    0.05  100000  |     0.00      0.43
2023-10-05 16:31:17,078 [INFO] price_array: 72540
2023-10-05 16:31:17,082 [INFO] | load actor from: ./trained_models/trainer4-4f340-steps-100000[512, 256]-2019-01-01/actor.pth
2023-10-05 16:31:27,096 [INFO] Test Finished!
2023-10-05 16:31:27,096 [INFO] episode_return: 1.0492429739865308
2023-10-05 17:22:44,761 [INFO] 
| `step`: Number of samples, or total training steps, or running times of `env.step()`.
| `time`: Time spent from the start of training to this moment.
| `avgR`: Average value of cumulative rewards, which is the sum of rewards in an episode.
| `stdR`: Standard dev of cumulative rewards, which is the sum of rewards in an episode.
| `avgS`: Average of steps in an episode.
| `objC`: Objective of Critic network. Or call it loss function of critic network.
| `objA`: Objective of Actor network. It is the average Q value of the critic network.
|     step      time  |     avgR    stdR    avgS  |     objC      objA
2023-10-05 17:29:38,769 [INFO] | 2.00e+04       414  |    -1.04    5.15  300000  |     0.00      0.43
2023-10-05 17:36:25,166 [INFO] | 4.00e+04       820  |    -2.14    3.44  300000  |     0.00      0.43
2023-10-05 17:43:10,881 [INFO] | 6.00e+04      1226  |    -1.44   27.79  300000  |     0.00      0.42
2023-10-05 17:49:38,146 [INFO] | 8.00e+04      1613  |     2.15   15.67  300000  |     0.00      0.43
2023-10-05 17:55:59,905 [INFO] | 1.00e+05      1995  |    18.33   15.97  300000  |     0.00      0.42
2023-10-05 18:04:36,677 [INFO] price_array: 72540
2023-10-05 18:04:36,680 [INFO] | load actor from: ./trained_models/trainer4-25c28-steps-300000[512, 256, 128]-2019-01-01/actor.pth
2023-10-05 18:04:47,003 [INFO] Test Finished!
2023-10-05 18:04:47,005 [INFO] episode_return: 1.8261091053934098
2023-10-05 18:54:14,138 [INFO] 
| `step`: Number of samples, or total training steps, or running times of `env.step()`.
| `time`: Time spent from the start of training to this moment.
| `avgR`: Average value of cumulative rewards, which is the sum of rewards in an episode.
| `stdR`: Standard dev of cumulative rewards, which is the sum of rewards in an episode.
| `avgS`: Average of steps in an episode.
| `objC`: Objective of Critic network. Or call it loss function of critic network.
| `objA`: Objective of Actor network. It is the average Q value of the critic network.
|     step      time  |     avgR    stdR    avgS  |     objC      objA
2023-10-05 18:58:20,412 [INFO] | 2.00e+04       246  |     7.18    3.88  200000  |     0.00      0.42
2023-10-05 19:02:23,422 [INFO] | 4.00e+04       489  |     8.15    3.65  200000  |     0.00      0.43
2023-10-05 19:06:17,193 [INFO] | 6.00e+04       723  |     6.16    5.51  200000  |     0.00      0.43
2023-10-05 19:10:13,364 [INFO] | 8.00e+04       959  |     9.17    5.02  200000  |     0.00      0.43
2023-10-05 19:14:11,708 [INFO] | 1.00e+05      1198  |     7.66    4.11  200000  |     0.00      0.42
2023-10-05 19:23:37,457 [INFO] price_array: 72540
2023-10-05 19:23:37,459 [INFO] | load actor from: ./trained_models/trainer4-66541-steps-200000[128, 64, 32]-2019-01-01/actor.pth
2023-10-05 19:23:47,590 [INFO] Test Finished!
2023-10-05 19:23:47,591 [INFO] episode_return: 1.5471127004034053
2023-10-05 20:17:14,571 [INFO] 
| `step`: Number of samples, or total training steps, or running times of `env.step()`.
| `time`: Time spent from the start of training to this moment.
| `avgR`: Average value of cumulative rewards, which is the sum of rewards in an episode.
| `stdR`: Standard dev of cumulative rewards, which is the sum of rewards in an episode.
| `avgS`: Average of steps in an episode.
| `objC`: Objective of Critic network. Or call it loss function of critic network.
| `objA`: Objective of Actor network. It is the average Q value of the critic network.
|     step      time  |     avgR    stdR    avgS  |     objC      objA
2023-10-05 20:23:36,291 [INFO] | 2.00e+04       382  |    18.41   23.88  300000  |     0.01      0.41
2023-10-05 20:29:52,875 [INFO] | 4.00e+04       758  |    27.14   24.04  300000  |     0.00      0.43
2023-10-05 20:36:10,261 [INFO] | 6.00e+04      1136  |    25.39   29.34  300000  |     0.00      0.43
2023-10-05 20:42:24,348 [INFO] | 8.00e+04      1510  |    -5.33   26.52  300000  |     0.00      0.43
2023-10-05 20:48:25,741 [INFO] | 1.00e+05      1871  |    12.08   32.34  300000  |     0.00      0.42
2023-10-05 20:58:17,842 [INFO] price_array: 72540
2023-10-05 20:58:17,846 [INFO] | load actor from: ./trained_models/trainer4-91ca6-steps-300000[512, 256]-2019-01-01/actor.pth
2023-10-05 20:58:27,589 [INFO] Test Finished!
2023-10-05 20:58:27,589 [INFO] episode_return: 1.1947145444050342
2023-10-05 21:53:40,724 [INFO] 
| `step`: Number of samples, or total training steps, or running times of `env.step()`.
| `time`: Time spent from the start of training to this moment.
| `avgR`: Average value of cumulative rewards, which is the sum of rewards in an episode.
| `stdR`: Standard dev of cumulative rewards, which is the sum of rewards in an episode.
| `avgS`: Average of steps in an episode.
| `objC`: Objective of Critic network. Or call it loss function of critic network.
| `objA`: Objective of Actor network. It is the average Q value of the critic network.
|     step      time  |     avgR    stdR    avgS  |     objC      objA
2023-10-05 21:58:57,909 [INFO] | 2.00e+04       317  |   -14.14    7.75  300000  |     0.00      0.43
2023-10-05 22:04:04,983 [INFO] | 4.00e+04       624  |   -10.98    8.24  300000  |     0.00      0.44
2023-10-05 22:09:08,922 [INFO] | 6.00e+04       928  |   -11.63   11.68  300000  |     0.00      0.43
2023-10-05 22:14:16,798 [INFO] | 8.00e+04      1236  |    -5.48    6.93  300000  |     0.00      0.44
2023-10-05 22:19:26,007 [INFO] | 1.00e+05      1545  |     1.74    3.15  300000  |     0.00      0.44
2023-10-05 22:29:32,448 [INFO] price_array: 72540
2023-10-05 22:29:32,451 [INFO] | load actor from: ./trained_models/trainer4-3628c-steps-300000[128, 64]-2019-01-01/actor.pth
2023-10-05 22:29:41,496 [INFO] Test Finished!
2023-10-05 22:29:41,496 [INFO] episode_return: 0.8923821324703708
2023-10-05 23:23:32,214 [INFO] 
| `step`: Number of samples, or total training steps, or running times of `env.step()`.
| `time`: Time spent from the start of training to this moment.
| `avgR`: Average value of cumulative rewards, which is the sum of rewards in an episode.
| `stdR`: Standard dev of cumulative rewards, which is the sum of rewards in an episode.
| `avgS`: Average of steps in an episode.
| `objC`: Objective of Critic network. Or call it loss function of critic network.
| `objA`: Objective of Actor network. It is the average Q value of the critic network.
|     step      time  |     avgR    stdR    avgS  |     objC      objA
2023-10-05 23:25:31,662 [INFO] | 2.00e+04       119  |     0.18    0.13  100000  |     0.00      0.41
2023-10-05 23:27:29,931 [INFO] | 4.00e+04       238  |     0.00    0.10  100000  |     0.00      0.43
2023-10-05 23:29:25,344 [INFO] | 6.00e+04       353  |    -0.01    0.13  100000  |     0.00      0.43
2023-10-05 23:31:21,421 [INFO] | 8.00e+04       469  |    -0.01    0.09  100000  |     0.00      0.43
2023-10-05 23:33:17,022 [INFO] | 1.00e+05       585  |    -0.04    0.12  100000  |     0.00      0.44
2023-10-05 23:41:47,845 [INFO] price_array: 72540
2023-10-05 23:41:47,848 [INFO] | load actor from: ./trained_models/trainer4-77df7-steps-100000[128, 64, 32]-2019-01-01/actor.pth
2023-10-05 23:41:57,086 [INFO] Test Finished!
2023-10-05 23:41:57,087 [INFO] episode_return: 1.2386589927450191
2023-10-06 00:31:09,289 [INFO] 
| `step`: Number of samples, or total training steps, or running times of `env.step()`.
| `time`: Time spent from the start of training to this moment.
| `avgR`: Average value of cumulative rewards, which is the sum of rewards in an episode.
| `stdR`: Standard dev of cumulative rewards, which is the sum of rewards in an episode.
| `avgS`: Average of steps in an episode.
| `objC`: Objective of Critic network. Or call it loss function of critic network.
| `objA`: Objective of Actor network. It is the average Q value of the critic network.
|     step      time  |     avgR    stdR    avgS  |     objC      objA
2023-10-06 00:35:25,057 [INFO] | 2.00e+04       256  |   -17.38    4.04  200000  |     0.00      0.41
2023-10-06 00:39:27,074 [INFO] | 4.00e+04       498  |   -16.15    7.26  200000  |     0.00      0.42
2023-10-06 00:43:27,774 [INFO] | 6.00e+04       738  |   -12.77    5.48  200000  |     0.00      0.43
2023-10-06 00:47:32,163 [INFO] | 8.00e+04       983  |   -13.03    2.68  200000  |     0.00      0.43
2023-10-06 00:51:22,516 [INFO] | 1.00e+05      1213  |     0.12    2.17  200000  |     0.00      0.45
2023-10-06 00:59:57,492 [INFO] price_array: 72540
2023-10-06 00:59:57,496 [INFO] | load actor from: ./trained_models/trainer4-c9d11-steps-200000[512, 256]-2019-01-01/actor.pth
2023-10-06 01:00:06,861 [INFO] Test Finished!
2023-10-06 01:00:06,862 [INFO] episode_return: 1.2199856576045154
2023-10-06 01:49:40,119 [INFO] 
| `step`: Number of samples, or total training steps, or running times of `env.step()`.
| `time`: Time spent from the start of training to this moment.
| `avgR`: Average value of cumulative rewards, which is the sum of rewards in an episode.
| `stdR`: Standard dev of cumulative rewards, which is the sum of rewards in an episode.
| `avgS`: Average of steps in an episode.
| `objC`: Objective of Critic network. Or call it loss function of critic network.
| `objA`: Objective of Actor network. It is the average Q value of the critic network.
|     step      time  |     avgR    stdR    avgS  |     objC      objA
2023-10-06 01:51:30,239 [INFO] | 2.00e+04       110  |    -0.07    0.19  100000  |     0.00      0.41
2023-10-06 01:53:12,982 [INFO] | 4.00e+04       213  |    -0.12    0.09  100000  |     0.00      0.45
2023-10-06 01:54:58,672 [INFO] | 6.00e+04       319  |    -0.06    0.16  100000  |     0.00      0.42
2023-10-06 01:56:49,910 [INFO] | 8.00e+04       430  |    -0.16    0.19  100000  |     0.00      0.44
2023-10-06 01:58:41,708 [INFO] | 1.00e+05       542  |    -0.12    0.21  100000  |     0.00      0.42
2023-10-06 02:07:14,684 [INFO] price_array: 72540
2023-10-06 02:07:14,688 [INFO] | load actor from: ./trained_models/trainer4-75ff8-steps-100000[256, 128, 64]-2019-01-01/actor.pth
2023-10-06 02:07:23,211 [INFO] Test Finished!
2023-10-06 02:07:23,212 [INFO] episode_return: 0.9914050937860497
2023-10-06 02:56:56,604 [INFO] 
| `step`: Number of samples, or total training steps, or running times of `env.step()`.
| `time`: Time spent from the start of training to this moment.
| `avgR`: Average value of cumulative rewards, which is the sum of rewards in an episode.
| `stdR`: Standard dev of cumulative rewards, which is the sum of rewards in an episode.
| `avgS`: Average of steps in an episode.
| `objC`: Objective of Critic network. Or call it loss function of critic network.
| `objA`: Objective of Actor network. It is the average Q value of the critic network.
|     step      time  |     avgR    stdR    avgS  |     objC      objA
2023-10-06 03:01:31,065 [INFO] | 2.00e+04       274  |     3.76    3.15  200000  |     0.00      0.41
2023-10-06 03:05:56,169 [INFO] | 4.00e+04       540  |     5.67    2.82  200000  |     0.00      0.44
2023-10-06 03:10:19,556 [INFO] | 6.00e+04       803  |     4.75    1.90  200000  |     0.00      0.43
2023-10-06 03:14:26,083 [INFO] | 8.00e+04      1049  |     5.86    3.16  200000  |     0.00      0.42
2023-10-06 03:18:29,502 [INFO] | 1.00e+05      1293  |     5.37    2.55  200000  |     0.00      0.44
2023-10-06 03:27:40,376 [INFO] price_array: 72540
2023-10-06 03:27:40,380 [INFO] | load actor from: ./trained_models/trainer4-d875e-steps-200000[512, 256]-2019-01-01/actor.pth
2023-10-06 03:27:50,078 [INFO] Test Finished!
2023-10-06 03:27:50,079 [INFO] episode_return: 1.348465352132273
2023-10-06 04:18:05,363 [INFO] 
| `step`: Number of samples, or total training steps, or running times of `env.step()`.
| `time`: Time spent from the start of training to this moment.
| `avgR`: Average value of cumulative rewards, which is the sum of rewards in an episode.
| `stdR`: Standard dev of cumulative rewards, which is the sum of rewards in an episode.
| `avgS`: Average of steps in an episode.
| `objC`: Objective of Critic network. Or call it loss function of critic network.
| `objA`: Objective of Actor network. It is the average Q value of the critic network.
|     step      time  |     avgR    stdR    avgS  |     objC      objA
2023-10-06 04:20:11,595 [INFO] | 2.00e+04       126  |    -0.04    0.05  100000  |     0.00      0.42
2023-10-06 04:22:14,245 [INFO] | 4.00e+04       249  |    -0.10    0.06  100000  |     0.00      0.43
2023-10-06 04:24:16,639 [INFO] | 6.00e+04       371  |    -0.07    0.08  100000  |     0.00      0.41
2023-10-06 04:26:19,532 [INFO] | 8.00e+04       494  |    -0.11    0.10  100000  |     0.00      0.43
2023-10-06 04:28:22,908 [INFO] | 1.00e+05       618  |    -0.06    0.08  100000  |     0.00      0.43
2023-10-06 04:36:41,148 [INFO] price_array: 72540
2023-10-06 04:36:41,153 [INFO] | load actor from: ./trained_models/trainer4-8a523-steps-100000[512, 256, 128]-2019-01-01/actor.pth
2023-10-06 04:36:50,948 [INFO] Test Finished!
2023-10-06 04:36:50,949 [INFO] episode_return: 1.7038226927475457
2023-10-06 05:25:13,704 [INFO] 
| `step`: Number of samples, or total training steps, or running times of `env.step()`.
| `time`: Time spent from the start of training to this moment.
| `avgR`: Average value of cumulative rewards, which is the sum of rewards in an episode.
| `stdR`: Standard dev of cumulative rewards, which is the sum of rewards in an episode.
| `avgS`: Average of steps in an episode.
| `objC`: Objective of Critic network. Or call it loss function of critic network.
| `objA`: Objective of Actor network. It is the average Q value of the critic network.
|     step      time  |     avgR    stdR    avgS  |     objC      objA
2023-10-06 05:30:39,235 [INFO] | 2.00e+04       326  |   -17.00    5.76  200000  |     0.00      0.41
2023-10-06 05:35:44,992 [INFO] | 4.00e+04       631  |   -15.82    6.21  200000  |     0.00      0.43
2023-10-06 05:40:49,149 [INFO] | 6.00e+04       935  |   -18.95    3.42  200000  |     0.00      0.44
2023-10-06 05:46:19,601 [INFO] | 8.00e+04      1266  |   -16.99    3.99  200000  |     0.00      0.41
2023-10-06 05:51:28,672 [INFO] | 1.00e+05      1575  |   -16.41    4.04  200000  |     0.00      0.43
2023-10-06 05:59:47,606 [INFO] price_array: 72540
2023-10-06 05:59:47,614 [INFO] | load actor from: ./trained_models/trainer4-144a3-steps-200000[1024, 512]-2019-01-01/actor.pth
2023-10-06 06:00:00,068 [INFO] Test Finished!
2023-10-06 06:00:00,068 [INFO] episode_return: 1.1543256398991597
2023-10-06 06:46:46,971 [INFO] 
| `step`: Number of samples, or total training steps, or running times of `env.step()`.
| `time`: Time spent from the start of training to this moment.
| `avgR`: Average value of cumulative rewards, which is the sum of rewards in an episode.
| `stdR`: Standard dev of cumulative rewards, which is the sum of rewards in an episode.
| `avgS`: Average of steps in an episode.
| `objC`: Objective of Critic network. Or call it loss function of critic network.
| `objA`: Objective of Actor network. It is the average Q value of the critic network.
|     step      time  |     avgR    stdR    avgS  |     objC      objA
2023-10-06 06:48:35,399 [INFO] | 2.00e+04       108  |     0.04    0.03  100000  |     0.00      0.41
2023-10-06 06:50:24,522 [INFO] | 4.00e+04       218  |     0.03    0.04  100000  |     0.00      0.42
2023-10-06 06:52:08,083 [INFO] | 6.00e+04       321  |    -0.04    0.07  100000  |     0.00      0.42
2023-10-06 06:53:52,003 [INFO] | 8.00e+04       425  |     0.01    0.07  100000  |     0.00      0.42
2023-10-06 06:55:34,797 [INFO] | 1.00e+05       528  |    -0.00    0.07  100000  |     0.00      0.42
2023-10-06 07:03:46,680 [INFO] price_array: 72540
2023-10-06 07:03:46,683 [INFO] | load actor from: ./trained_models/trainer4-52634-steps-100000[128, 64]-2019-01-01/actor.pth
2023-10-06 07:03:54,944 [INFO] Test Finished!
2023-10-06 07:03:54,945 [INFO] episode_return: 1.5230227205578366
2023-10-06 07:53:00,255 [INFO] 
| `step`: Number of samples, or total training steps, or running times of `env.step()`.
| `time`: Time spent from the start of training to this moment.
| `avgR`: Average value of cumulative rewards, which is the sum of rewards in an episode.
| `stdR`: Standard dev of cumulative rewards, which is the sum of rewards in an episode.
| `avgS`: Average of steps in an episode.
| `objC`: Objective of Critic network. Or call it loss function of critic network.
| `objA`: Objective of Actor network. It is the average Q value of the critic network.
|     step      time  |     avgR    stdR    avgS  |     objC      objA
2023-10-06 07:57:18,950 [INFO] | 2.00e+04       259  |     1.36    5.62  200000  |     0.00      0.43
2023-10-06 08:01:38,739 [INFO] | 4.00e+04       518  |     0.68    5.08  200000  |     0.00      0.43
2023-10-06 08:06:01,960 [INFO] | 6.00e+04       782  |     3.65    4.37  200000  |     0.00      0.43
2023-10-06 08:10:21,441 [INFO] | 8.00e+04      1041  |     0.97    3.58  200000  |     0.00      0.43
2023-10-06 08:14:34,699 [INFO] | 1.00e+05      1294  |     1.43    4.42  200000  |     0.00      0.43
2023-10-06 08:22:58,358 [INFO] price_array: 72540
2023-10-06 08:22:58,361 [INFO] | load actor from: ./trained_models/trainer4-6bc4b-steps-200000[128, 64, 32]-2019-01-01/actor.pth
2023-10-06 08:23:07,603 [INFO] Test Finished!
2023-10-06 08:23:07,605 [INFO] episode_return: 0.8340861943322667
2023-10-06 09:10:28,083 [INFO] 
| `step`: Number of samples, or total training steps, or running times of `env.step()`.
| `time`: Time spent from the start of training to this moment.
| `avgR`: Average value of cumulative rewards, which is the sum of rewards in an episode.
| `stdR`: Standard dev of cumulative rewards, which is the sum of rewards in an episode.
| `avgS`: Average of steps in an episode.
| `objC`: Objective of Critic network. Or call it loss function of critic network.
| `objA`: Objective of Actor network. It is the average Q value of the critic network.
|     step      time  |     avgR    stdR    avgS  |     objC      objA
2023-10-06 09:14:51,952 [INFO] | 2.00e+04       264  |   -23.47   12.06  200000  |     0.00      0.44
2023-10-06 09:19:04,076 [INFO] | 4.00e+04       516  |   -32.78   22.51  200000  |     0.00      0.43
2023-10-06 09:23:20,254 [INFO] | 6.00e+04       772  |   -11.33   16.60  200000  |     0.00      0.43
2023-10-06 09:27:30,701 [INFO] | 8.00e+04      1023  |   -24.52   20.36  200000  |     0.00      0.42
2023-10-06 09:31:37,196 [INFO] | 1.00e+05      1269  |   -31.01   17.31  200000  |     0.00      0.43
2023-10-06 09:43:55,872 [INFO] price_array: 72540
2023-10-06 09:43:55,875 [INFO] | load actor from: ./trained_models/trainer4-00411-steps-200000[256, 128]-2019-01-01/actor.pth
2023-10-06 09:44:04,968 [INFO] Test Finished!
2023-10-06 09:44:04,968 [INFO] episode_return: 1.1578933421886932
2023-10-06 10:36:15,876 [INFO] 
| `step`: Number of samples, or total training steps, or running times of `env.step()`.
| `time`: Time spent from the start of training to this moment.
| `avgR`: Average value of cumulative rewards, which is the sum of rewards in an episode.
| `stdR`: Standard dev of cumulative rewards, which is the sum of rewards in an episode.
| `avgS`: Average of steps in an episode.
| `objC`: Objective of Critic network. Or call it loss function of critic network.
| `objA`: Objective of Actor network. It is the average Q value of the critic network.
|     step      time  |     avgR    stdR    avgS  |     objC      objA
2023-10-06 10:40:44,891 [INFO] | 2.00e+04       269  |   -10.36    3.68  200000  |     0.00      0.43
2023-10-06 10:45:15,617 [INFO] | 4.00e+04       540  |   -10.33    2.64  200000  |     0.00      0.43
2023-10-06 10:49:48,150 [INFO] | 6.00e+04       812  |   -10.98    3.25  200000  |     0.00      0.41
2023-10-06 10:54:13,116 [INFO] | 8.00e+04      1077  |   -10.49    3.90  200000  |     0.00      0.42
2023-10-06 10:58:26,416 [INFO] | 1.00e+05      1331  |   -11.56    5.08  200000  |     0.00      0.43
2023-10-06 11:07:43,508 [INFO] price_array: 72540
2023-10-06 11:07:43,510 [INFO] | load actor from: ./trained_models/trainer4-68d7d-steps-200000[256, 128, 64]-2019-01-01/actor.pth
2023-10-06 11:07:52,544 [INFO] Test Finished!
2023-10-06 11:07:52,545 [INFO] episode_return: 1.2669337916245527
2023-10-06 11:58:08,489 [INFO] 
| `step`: Number of samples, or total training steps, or running times of `env.step()`.
| `time`: Time spent from the start of training to this moment.
| `avgR`: Average value of cumulative rewards, which is the sum of rewards in an episode.
| `stdR`: Standard dev of cumulative rewards, which is the sum of rewards in an episode.
| `avgS`: Average of steps in an episode.
| `objC`: Objective of Critic network. Or call it loss function of critic network.
| `objA`: Objective of Actor network. It is the average Q value of the critic network.
|     step      time  |     avgR    stdR    avgS  |     objC      objA
2023-10-06 12:00:20,136 [INFO] | 2.00e+04       132  |     2.81    1.24  100000  |     0.00      0.43
2023-10-06 12:02:32,558 [INFO] | 4.00e+04       264  |     4.51    2.55  100000  |     0.00      0.42
2023-10-06 12:04:45,388 [INFO] | 6.00e+04       397  |     4.20    0.65  100000  |     0.00      0.42
2023-10-06 12:06:57,316 [INFO] | 8.00e+04       529  |     2.18    0.51  100000  |     0.00      0.43
2023-10-06 12:09:07,597 [INFO] | 1.00e+05       659  |     0.43    0.28  100000  |     0.00      0.42
2023-10-06 12:18:23,043 [INFO] price_array: 72540
2023-10-06 12:18:23,047 [INFO] | load actor from: ./trained_models/trainer4-8f612-steps-100000[512, 256]-2019-01-01/actor.pth
2023-10-06 12:18:33,815 [INFO] Test Finished!
2023-10-06 12:18:33,815 [INFO] episode_return: 0.6225815138384825
2023-10-06 13:09:26,480 [INFO] 
| `step`: Number of samples, or total training steps, or running times of `env.step()`.
| `time`: Time spent from the start of training to this moment.
| `avgR`: Average value of cumulative rewards, which is the sum of rewards in an episode.
| `stdR`: Standard dev of cumulative rewards, which is the sum of rewards in an episode.
| `avgS`: Average of steps in an episode.
| `objC`: Objective of Critic network. Or call it loss function of critic network.
| `objA`: Objective of Actor network. It is the average Q value of the critic network.
|     step      time  |     avgR    stdR    avgS  |     objC      objA
2023-10-06 13:15:49,229 [INFO] | 2.00e+04       383  |    -9.33    6.50  300000  |     0.00      0.43
2023-10-06 13:22:16,266 [INFO] | 4.00e+04       770  |   -17.94   10.39  300000  |     0.00      0.42
2023-10-06 13:28:41,978 [INFO] | 6.00e+04      1155  |    -8.16    3.67  300000  |     0.00      0.42
2023-10-06 13:35:18,941 [INFO] | 8.00e+04      1552  |   -11.72    7.36  300000  |     0.00      0.43
2023-10-06 13:41:50,368 [INFO] | 1.00e+05      1944  |   -23.81   30.52  300000  |     0.00      0.42
2023-10-06 13:50:57,259 [INFO] price_array: 72540
2023-10-06 13:50:57,263 [INFO] | load actor from: ./trained_models/trainer4-f40ad-steps-300000[256, 128, 64]-2019-01-01/actor.pth
2023-10-06 13:51:06,840 [INFO] Test Finished!
2023-10-06 13:51:06,841 [INFO] episode_return: 0.8968579368484023
2023-10-06 16:34:51,046 [INFO] 
| `step`: Number of samples, or total training steps, or running times of `env.step()`.
| `time`: Time spent from the start of training to this moment.
| `avgR`: Average value of cumulative rewards, which is the sum of rewards in an episode.
| `stdR`: Standard dev of cumulative rewards, which is the sum of rewards in an episode.
| `avgS`: Average of steps in an episode.
| `objC`: Objective of Critic network. Or call it loss function of critic network.
| `objA`: Objective of Actor network. It is the average Q value of the critic network.
|     step      time  |     avgR    stdR    avgS  |     objC      objA
2023-10-06 16:41:17,218 [INFO] | 2.00e+04       386  |    -0.70   18.50  300000  |     0.00      0.41
2023-10-06 16:47:36,032 [INFO] | 4.00e+04       765  |     0.38   12.40  300000  |     0.00      0.43
2023-10-06 16:54:04,424 [INFO] | 6.00e+04      1153  |    10.87   10.90  300000  |     0.00      0.42
2023-10-06 17:00:27,787 [INFO] | 8.00e+04      1537  |     0.43   10.88  300000  |     0.00      0.43
2023-10-06 17:06:40,442 [INFO] | 1.00e+05      1909  |     9.29   18.47  300000  |     0.00      0.43
2023-10-06 17:15:02,244 [INFO] price_array: 72540
2023-10-06 17:15:02,247 [INFO] | load actor from: ./trained_models/trainer4-5641f-steps-300000[256, 128]-2019-01-01/actor.pth
2023-10-06 17:15:11,981 [INFO] Test Finished!
2023-10-06 17:15:11,982 [INFO] episode_return: 1.467742711495257
2023-10-06 18:03:53,345 [INFO] 
| `step`: Number of samples, or total training steps, or running times of `env.step()`.
| `time`: Time spent from the start of training to this moment.
| `avgR`: Average value of cumulative rewards, which is the sum of rewards in an episode.
| `stdR`: Standard dev of cumulative rewards, which is the sum of rewards in an episode.
| `avgS`: Average of steps in an episode.
| `objC`: Objective of Critic network. Or call it loss function of critic network.
| `objA`: Objective of Actor network. It is the average Q value of the critic network.
|     step      time  |     avgR    stdR    avgS  |     objC      objA
2023-10-06 18:08:17,537 [INFO] | 2.00e+04       264  |     0.38    2.79  200000  |     0.00      0.42
2023-10-06 18:12:44,885 [INFO] | 4.00e+04       532  |     0.93    2.08  200000  |     0.00      0.43
2023-10-06 18:17:09,660 [INFO] | 6.00e+04       796  |     0.88    2.64  200000  |     0.00      0.41
2023-10-06 18:21:29,773 [INFO] | 8.00e+04      1056  |     1.51    2.64  200000  |     0.00      0.43
2023-10-06 18:25:56,914 [INFO] | 1.00e+05      1324  |     2.05    1.39  200000  |     0.00      0.43
2023-10-06 18:34:38,762 [INFO] price_array: 72540
2023-10-06 18:34:38,764 [INFO] | load actor from: ./trained_models/trainer4-c482b-steps-200000[256, 128, 64]-2019-01-01/actor.pth
2023-10-06 18:34:48,952 [INFO] Test Finished!
2023-10-06 18:34:48,953 [INFO] episode_return: 1.2457794218096256
2023-10-06 19:23:28,999 [INFO] 
| `step`: Number of samples, or total training steps, or running times of `env.step()`.
| `time`: Time spent from the start of training to this moment.
| `avgR`: Average value of cumulative rewards, which is the sum of rewards in an episode.
| `stdR`: Standard dev of cumulative rewards, which is the sum of rewards in an episode.
| `avgS`: Average of steps in an episode.
| `objC`: Objective of Critic network. Or call it loss function of critic network.
| `objA`: Objective of Actor network. It is the average Q value of the critic network.
|     step      time  |     avgR    stdR    avgS  |     objC      objA
2023-10-06 19:30:19,760 [INFO] | 2.00e+04       411  |    20.28   16.37  300000  |     0.00      0.43
2023-10-06 19:37:09,950 [INFO] | 4.00e+04       821  |    28.19   10.56  300000  |     0.00      0.43
2023-10-06 19:44:03,555 [INFO] | 6.00e+04      1235  |    18.95   15.82  300000  |     0.00      0.42
2023-10-06 19:50:59,221 [INFO] | 8.00e+04      1650  |    22.56   14.38  300000  |     0.00      0.42
2023-10-06 19:58:02,164 [INFO] | 1.00e+05      2073  |    23.15   11.35  300000  |     0.00      0.42
2023-10-06 20:06:42,130 [INFO] price_array: 72540
2023-10-06 20:06:42,133 [INFO] | load actor from: ./trained_models/trainer4-c7400-steps-300000[512, 256]-2019-01-01/actor.pth
2023-10-06 20:06:53,021 [INFO] Test Finished!
2023-10-06 20:06:53,022 [INFO] episode_return: 0.66117519467451
2023-10-06 20:55:09,368 [INFO] 
| `step`: Number of samples, or total training steps, or running times of `env.step()`.
| `time`: Time spent from the start of training to this moment.
| `avgR`: Average value of cumulative rewards, which is the sum of rewards in an episode.
| `stdR`: Standard dev of cumulative rewards, which is the sum of rewards in an episode.
| `avgS`: Average of steps in an episode.
| `objC`: Objective of Critic network. Or call it loss function of critic network.
| `objA`: Objective of Actor network. It is the average Q value of the critic network.
|     step      time  |     avgR    stdR    avgS  |     objC      objA
2023-10-06 20:59:29,142 [INFO] | 2.00e+04       260  |   -10.16    7.11  200000  |     0.00      0.43
2023-10-06 21:03:46,342 [INFO] | 4.00e+04       517  |    -2.88    9.47  200000  |     0.00      0.42
2023-10-06 21:08:08,875 [INFO] | 6.00e+04       780  |    -0.46    6.92  200000  |     0.00      0.42
2023-10-06 21:12:25,265 [INFO] | 8.00e+04      1036  |     0.33    1.28  200000  |     0.00      0.43
2023-10-06 21:16:45,457 [INFO] | 1.00e+05      1296  |    -3.63    5.76  200000  |     0.00      0.42
2023-10-06 21:25:55,820 [INFO] price_array: 72540
2023-10-06 21:25:55,849 [INFO] | load actor from: ./trained_models/trainer4-5cad8-steps-200000[512, 256, 128]-2019-01-01/actor.pth
2023-10-06 21:26:06,880 [INFO] Test Finished!
2023-10-06 21:26:06,880 [INFO] episode_return: 1.2714414269744907
2023-10-06 22:16:02,095 [INFO] 
| `step`: Number of samples, or total training steps, or running times of `env.step()`.
| `time`: Time spent from the start of training to this moment.
| `avgR`: Average value of cumulative rewards, which is the sum of rewards in an episode.
| `stdR`: Standard dev of cumulative rewards, which is the sum of rewards in an episode.
| `avgS`: Average of steps in an episode.
| `objC`: Objective of Critic network. Or call it loss function of critic network.
| `objA`: Objective of Actor network. It is the average Q value of the critic network.
|     step      time  |     avgR    stdR    avgS  |     objC      objA
2023-10-06 22:21:43,717 [INFO] | 2.00e+04       342  |    -3.35    2.32  200000  |     0.01      0.41
2023-10-06 22:27:26,157 [INFO] | 4.00e+04       684  |    -4.21    3.61  200000  |     0.00      0.43
2023-10-06 22:33:06,686 [INFO] | 6.00e+04      1025  |    -3.31    4.06  200000  |     0.00      0.43
2023-10-06 22:38:53,402 [INFO] | 8.00e+04      1371  |    -0.77    4.19  200000  |     0.00      0.44
2023-10-06 22:44:55,665 [INFO] | 1.00e+05      1734  |    -1.75    2.79  200000  |     0.00      0.43
2023-10-06 22:53:30,344 [INFO] price_array: 72540
2023-10-06 22:53:30,351 [INFO] | load actor from: ./trained_models/trainer4-ca522-steps-200000[1024, 512]-2019-01-01/actor.pth
2023-10-06 22:53:44,307 [INFO] Test Finished!
2023-10-06 22:53:44,307 [INFO] episode_return: 0.848226272138605
2023-10-06 23:24:08,292 [INFO] 
| `step`: Number of samples, or total training steps, or running times of `env.step()`.
| `time`: Time spent from the start of training to this moment.
| `avgR`: Average value of cumulative rewards, which is the sum of rewards in an episode.
| `stdR`: Standard dev of cumulative rewards, which is the sum of rewards in an episode.
| `avgS`: Average of steps in an episode.
| `objC`: Objective of Critic network. Or call it loss function of critic network.
| `objA`: Objective of Actor network. It is the average Q value of the critic network.
|     step      time  |     avgR    stdR    avgS  |     objC      objA
2023-10-06 23:28:08,487 [INFO] | 2.00e+04       240  |    11.85    3.33  200000  |     0.01      0.41
2023-10-06 23:32:07,892 [INFO] | 4.00e+04       480  |    12.13    2.24  200000  |     0.01      0.42
2023-10-06 23:36:05,913 [INFO] | 6.00e+04       718  |    13.33    2.42  200000  |     0.01      0.43
2023-10-06 23:40:14,766 [INFO] | 8.00e+04       966  |    11.38    2.08  200000  |     0.01      0.43
2023-10-06 23:44:21,193 [INFO] | 1.00e+05      1213  |    13.13    2.27  200000  |     0.01      0.43
2023-10-06 23:49:48,440 [INFO] price_array: 72540
2023-10-06 23:49:48,443 [INFO] | load actor from: ./trained_models/trainer4-d9ff4-steps-200000[128, 64]-2019-01-01/actor.pth
2023-10-06 23:49:58,457 [INFO] Test Finished!
2023-10-06 23:49:58,457 [INFO] episode_return: 1.0358244829305174
2023-10-07 00:22:08,013 [INFO] 
| `step`: Number of samples, or total training steps, or running times of `env.step()`.
| `time`: Time spent from the start of training to this moment.
| `avgR`: Average value of cumulative rewards, which is the sum of rewards in an episode.
| `stdR`: Standard dev of cumulative rewards, which is the sum of rewards in an episode.
| `avgS`: Average of steps in an episode.
| `objC`: Objective of Critic network. Or call it loss function of critic network.
| `objA`: Objective of Actor network. It is the average Q value of the critic network.
|     step      time  |     avgR    stdR    avgS  |     objC      objA
2023-10-07 00:26:27,553 [INFO] | 2.00e+04       260  |     6.32    7.38  200000  |     0.00      0.43
2023-10-07 00:30:34,288 [INFO] | 4.00e+04       506  |     8.65    6.90  200000  |     0.00      0.43
2023-10-07 00:34:45,858 [INFO] | 6.00e+04       758  |     8.44    4.23  200000  |     0.00      0.42
2023-10-07 00:38:58,001 [INFO] | 8.00e+04      1010  |     8.35    4.67  200000  |     0.00      0.43
2023-10-07 00:43:09,662 [INFO] | 1.00e+05      1262  |     6.65    4.73  200000  |     0.00      0.44
2023-10-07 00:48:20,299 [INFO] price_array: 72540
2023-10-07 00:48:20,303 [INFO] | load actor from: ./trained_models/trainer4-bee78-steps-200000[128, 64, 32]-2019-01-01/actor.pth
2023-10-07 00:48:30,914 [INFO] Test Finished!
2023-10-07 00:48:30,915 [INFO] episode_return: 1.0093096217058208
2023-10-07 01:35:13,542 [INFO] 
| `step`: Number of samples, or total training steps, or running times of `env.step()`.
| `time`: Time spent from the start of training to this moment.
| `avgR`: Average value of cumulative rewards, which is the sum of rewards in an episode.
| `stdR`: Standard dev of cumulative rewards, which is the sum of rewards in an episode.
| `avgS`: Average of steps in an episode.
| `objC`: Objective of Critic network. Or call it loss function of critic network.
| `objA`: Objective of Actor network. It is the average Q value of the critic network.
|     step      time  |     avgR    stdR    avgS  |     objC      objA
2023-10-07 01:43:26,628 [INFO] | 2.00e+04       493  |    -0.45    6.42  400000  |     0.00      0.43
2023-10-07 01:51:23,007 [INFO] | 4.00e+04       969  |     1.57    7.77  400000  |     0.00      0.44
2023-10-07 01:59:55,019 [INFO] | 6.00e+04      1481  |     4.00    3.04  400000  |     0.00      0.43
2023-10-07 02:08:43,628 [INFO] | 8.00e+04      2010  |    -1.66    4.53  400000  |     0.00      0.43
2023-10-07 02:17:56,838 [INFO] | 1.00e+05      2563  |    -5.65    3.71  400000  |     0.00      0.43
2023-10-07 02:23:27,133 [INFO] price_array: 72540
2023-10-07 02:23:27,136 [INFO] | load actor from: ./trained_models/trainer4-98eb7-steps-400000[256, 128]-2017-01-01/actor.pth
2023-10-07 02:23:37,421 [INFO] Test Finished!
2023-10-07 02:23:37,421 [INFO] episode_return: 1.3323735868122109
2023-10-07 03:12:11,041 [INFO] 
| `step`: Number of samples, or total training steps, or running times of `env.step()`.
| `time`: Time spent from the start of training to this moment.
| `avgR`: Average value of cumulative rewards, which is the sum of rewards in an episode.
| `stdR`: Standard dev of cumulative rewards, which is the sum of rewards in an episode.
| `avgS`: Average of steps in an episode.
| `objC`: Objective of Critic network. Or call it loss function of critic network.
| `objA`: Objective of Actor network. It is the average Q value of the critic network.
|     step      time  |     avgR    stdR    avgS  |     objC      objA
2023-10-07 03:20:10,893 [INFO] | 2.00e+04       480  |    -2.41    2.37  400000  |     0.00      0.43
2023-10-07 03:28:04,106 [INFO] | 4.00e+04       953  |    -0.60    1.30  400000  |     0.00      0.42
2023-10-07 03:36:10,245 [INFO] | 6.00e+04      1439  |    -0.97    0.99  400000  |     0.00      0.44
2023-10-07 03:44:26,074 [INFO] | 8.00e+04      1935  |    -0.59    1.28  400000  |     0.00      0.43
2023-10-07 03:52:25,515 [INFO] | 1.00e+05      2414  |     0.51    1.15  400000  |     0.00      0.43
2023-10-07 03:57:41,520 [INFO] price_array: 72540
2023-10-07 03:57:41,523 [INFO] | load actor from: ./trained_models/trainer4-d03d4-steps-400000[256, 128, 64]-2017-01-01/actor.pth
2023-10-07 03:57:51,935 [INFO] Test Finished!
2023-10-07 03:57:51,935 [INFO] episode_return: 1.1517432055126668
2023-10-07 04:45:05,506 [INFO] 
| `step`: Number of samples, or total training steps, or running times of `env.step()`.
| `time`: Time spent from the start of training to this moment.
| `avgR`: Average value of cumulative rewards, which is the sum of rewards in an episode.
| `stdR`: Standard dev of cumulative rewards, which is the sum of rewards in an episode.
| `avgS`: Average of steps in an episode.
| `objC`: Objective of Critic network. Or call it loss function of critic network.
| `objA`: Objective of Actor network. It is the average Q value of the critic network.
|     step      time  |     avgR    stdR    avgS  |     objC      objA
2023-10-07 04:54:14,341 [INFO] | 2.00e+04       549  |    13.76    2.71  400000  |     0.00      0.42
2023-10-07 05:03:47,816 [INFO] | 4.00e+04      1122  |    -9.85    4.17  400000  |     0.00      0.45
2023-10-07 05:12:41,252 [INFO] | 6.00e+04      1656  |    -3.76    3.83  400000  |     0.00      0.42
2023-10-07 05:21:50,974 [INFO] | 8.00e+04      2205  |    -4.96    6.09  400000  |     0.00      0.43
2023-10-07 05:31:30,589 [INFO] | 1.00e+05      2785  |    -9.48    8.97  400000  |     0.00      0.43
2023-10-07 05:36:51,298 [INFO] price_array: 72540
2023-10-07 05:36:51,302 [INFO] | load actor from: ./trained_models/trainer4-042b2-steps-400000[512, 256]-2017-01-01/actor.pth
2023-10-07 05:37:02,524 [INFO] Test Finished!
2023-10-07 05:37:02,525 [INFO] episode_return: 1.510711106259871
2023-10-07 06:25:28,115 [INFO] 
| `step`: Number of samples, or total training steps, or running times of `env.step()`.
| `time`: Time spent from the start of training to this moment.
| `avgR`: Average value of cumulative rewards, which is the sum of rewards in an episode.
| `stdR`: Standard dev of cumulative rewards, which is the sum of rewards in an episode.
| `avgS`: Average of steps in an episode.
| `objC`: Objective of Critic network. Or call it loss function of critic network.
| `objA`: Objective of Actor network. It is the average Q value of the critic network.
|     step      time  |     avgR    stdR    avgS  |     objC      objA
2023-10-07 06:33:59,394 [INFO] | 2.00e+04       511  |    -1.53    1.52  400000  |     0.00      0.42
2023-10-07 06:42:20,348 [INFO] | 4.00e+04      1012  |     1.88    1.38  400000  |     0.00      0.43
2023-10-07 06:50:29,576 [INFO] | 6.00e+04      1501  |     3.07    2.70  400000  |     0.00      0.42
2023-10-07 06:59:02,831 [INFO] | 8.00e+04      2015  |    -0.13    2.64  400000  |     0.00      0.43
2023-10-07 07:07:45,798 [INFO] | 1.00e+05      2538  |    -6.22    4.12  400000  |     0.00      0.43
2023-10-07 07:12:52,729 [INFO] price_array: 72540
2023-10-07 07:12:52,734 [INFO] | load actor from: ./trained_models/trainer4-0e865-steps-400000[512, 256, 128]-2017-01-01/actor.pth
2023-10-07 07:13:03,981 [INFO] Test Finished!
2023-10-07 07:13:03,981 [INFO] episode_return: 0.9749312004985333
2023-10-07 08:00:52,164 [INFO] 
| `step`: Number of samples, or total training steps, or running times of `env.step()`.
| `time`: Time spent from the start of training to this moment.
| `avgR`: Average value of cumulative rewards, which is the sum of rewards in an episode.
| `stdR`: Standard dev of cumulative rewards, which is the sum of rewards in an episode.
| `avgS`: Average of steps in an episode.
| `objC`: Objective of Critic network. Or call it loss function of critic network.
| `objA`: Objective of Actor network. It is the average Q value of the critic network.
|     step      time  |     avgR    stdR    avgS  |     objC      objA
2023-10-07 08:12:12,474 [INFO] | 2.00e+04       680  |    -4.12    4.45  400000  |     0.00      0.43
2023-10-07 08:22:58,623 [INFO] | 4.00e+04      1326  |    -2.88    3.47  400000  |     0.00      0.44
2023-10-07 08:33:00,664 [INFO] | 6.00e+04      1929  |    -3.36    6.46  400000  |     0.00      0.43
2023-10-07 08:43:04,066 [INFO] | 8.00e+04      2532  |    -1.88    4.94  400000  |     0.00      0.43
2023-10-07 08:53:52,137 [INFO] | 1.00e+05      3180  |    -6.25    3.29  400000  |     0.00      0.41
2023-10-07 08:55:58,490 [WARNING] sleep 3 seconds and retrying https://data.alpaca.markets/v2/stocks/PYPL/bars 3 more time(s)...
2023-10-07 08:55:58,525 [WARNING] sleep 3 seconds and retrying https://data.alpaca.markets/v2/stocks/META/bars 3 more time(s)...
2023-10-07 08:55:58,525 [WARNING] sleep 3 seconds and retrying https://data.alpaca.markets/v2/stocks/JNJ/bars 3 more time(s)...
2023-10-07 08:55:58,541 [WARNING] sleep 3 seconds and retrying https://data.alpaca.markets/v2/stocks/CRM/bars 3 more time(s)...
2023-10-07 08:55:58,559 [WARNING] sleep 3 seconds and retrying https://data.alpaca.markets/v2/stocks/SPOT/bars 3 more time(s)...
2023-10-07 08:55:58,564 [WARNING] sleep 3 seconds and retrying https://data.alpaca.markets/v2/stocks/DIS/bars 3 more time(s)...
2023-10-07 08:56:02,431 [WARNING] sleep 3 seconds and retrying https://data.alpaca.markets/v2/stocks/META/bars 3 more time(s)...
2023-10-07 08:56:02,471 [WARNING] sleep 3 seconds and retrying https://data.alpaca.markets/v2/stocks/SPOT/bars 3 more time(s)...
2023-10-07 08:56:02,479 [WARNING] sleep 3 seconds and retrying https://data.alpaca.markets/v2/stocks/PFE/bars 3 more time(s)...
2023-10-07 08:56:02,480 [WARNING] sleep 3 seconds and retrying https://data.alpaca.markets/v2/stocks/SHOP/bars 3 more time(s)...
2023-10-07 08:56:02,541 [WARNING] sleep 3 seconds and retrying https://data.alpaca.markets/v2/stocks/SQ/bars 3 more time(s)...
2023-10-07 08:56:02,914 [WARNING] sleep 3 seconds and retrying https://data.alpaca.markets/v2/stocks/JNJ/bars 3 more time(s)...
2023-10-07 08:56:02,928 [WARNING] sleep 3 seconds and retrying https://data.alpaca.markets/v2/stocks/ROKU/bars 3 more time(s)...
2023-10-07 08:56:06,301 [WARNING] sleep 3 seconds and retrying https://data.alpaca.markets/v2/stocks/SQ/bars 3 more time(s)...
2023-10-07 08:56:06,369 [WARNING] sleep 3 seconds and retrying https://data.alpaca.markets/v2/stocks/PYPL/bars 3 more time(s)...
2023-10-07 08:56:06,369 [WARNING] sleep 3 seconds and retrying https://data.alpaca.markets/v2/stocks/PFE/bars 3 more time(s)...
2023-10-07 08:56:06,369 [WARNING] sleep 3 seconds and retrying https://data.alpaca.markets/v2/stocks/SPOT/bars 3 more time(s)...
2023-10-07 08:56:06,370 [WARNING] sleep 3 seconds and retrying https://data.alpaca.markets/v2/stocks/DIS/bars 3 more time(s)...
2023-10-07 08:56:06,407 [WARNING] sleep 3 seconds and retrying https://data.alpaca.markets/v2/stocks/SHOP/bars 3 more time(s)...
2023-10-07 08:56:08,117 [WARNING] sleep 3 seconds and retrying https://data.alpaca.markets/v2/stocks/META/bars 3 more time(s)...
2023-10-07 08:56:09,631 [WARNING] sleep 3 seconds and retrying https://data.alpaca.markets/v2/stocks/PFE/bars 3 more time(s)...
2023-10-07 08:56:09,670 [WARNING] sleep 3 seconds and retrying https://data.alpaca.markets/v2/stocks/DIS/bars 3 more time(s)...
2023-10-07 08:56:09,677 [WARNING] sleep 3 seconds and retrying https://data.alpaca.markets/v2/stocks/SQ/bars 3 more time(s)...
2023-10-07 08:56:09,677 [WARNING] sleep 3 seconds and retrying https://data.alpaca.markets/v2/stocks/PYPL/bars 3 more time(s)...
2023-10-07 08:56:15,756 [WARNING] sleep 3 seconds and retrying https://data.alpaca.markets/v2/stocks/META/bars 3 more time(s)...
2023-10-07 08:56:15,784 [WARNING] sleep 3 seconds and retrying https://data.alpaca.markets/v2/stocks/SHOP/bars 3 more time(s)...
2023-10-07 08:56:15,791 [WARNING] sleep 3 seconds and retrying https://data.alpaca.markets/v2/stocks/SQ/bars 3 more time(s)...
2023-10-07 08:56:15,797 [WARNING] sleep 3 seconds and retrying https://data.alpaca.markets/v2/stocks/PFE/bars 3 more time(s)...
2023-10-07 08:56:15,803 [WARNING] sleep 3 seconds and retrying https://data.alpaca.markets/v2/stocks/V/bars 3 more time(s)...
2023-10-07 08:56:15,835 [WARNING] sleep 3 seconds and retrying https://data.alpaca.markets/v2/stocks/PYPL/bars 3 more time(s)...
2023-10-07 08:56:16,438 [WARNING] sleep 3 seconds and retrying https://data.alpaca.markets/v2/stocks/DIS/bars 3 more time(s)...
