2023-10-03 14:15:41,567 [ERROR] packet queue is empty, aborting
2023-10-03 16:29:14,938 [ERROR] packet queue is empty, aborting
2023-10-03 17:22:59,367 [INFO] 
| `step`: Number of samples, or total training steps, or running times of `env.step()`.
| `time`: Time spent from the start of training to this moment.
| `avgR`: Average value of cumulative rewards, which is the sum of rewards in an episode.
| `stdR`: Standard dev of cumulative rewards, which is the sum of rewards in an episode.
| `avgS`: Average of steps in an episode.
| `objC`: Objective of Critic network. Or call it loss function of critic network.
| `objA`: Objective of Actor network. It is the average Q value of the critic network.
|     step      time  |     avgR    stdR    avgS  |     objC      objA
2023-10-03 17:25:42,477 [INFO] | 2.00e+04       163  |     0.00    0.00  300000  |     0.00      0.01
2023-10-03 17:28:26,050 [INFO] | 4.00e+04       327  |     0.00    0.00  300000  |     0.00      0.03
2023-10-03 17:31:13,504 [INFO] | 6.00e+04       494  |     0.00    0.00  300000  |     0.00      0.02
2023-10-03 17:33:56,592 [INFO] | 8.00e+04       657  |     0.00    0.00  300000  |     0.00      0.01
2023-10-03 17:36:39,076 [INFO] | 1.00e+05       820  |     0.00    0.00  300000  |     0.00      0.02
2023-10-03 17:39:06,324 [INFO] 
| `step`: Number of samples, or total training steps, or running times of `env.step()`.
| `time`: Time spent from the start of training to this moment.
| `avgR`: Average value of cumulative rewards, which is the sum of rewards in an episode.
| `stdR`: Standard dev of cumulative rewards, which is the sum of rewards in an episode.
| `avgS`: Average of steps in an episode.
| `objC`: Objective of Critic network. Or call it loss function of critic network.
| `objA`: Objective of Actor network. It is the average Q value of the critic network.
|     step      time  |     avgR    stdR    avgS  |     objC      objA
2023-10-03 17:41:32,863 [INFO] | 2.00e+04       147  |    -0.00    0.00  200000  |     0.00      0.01
2023-10-03 17:43:52,026 [INFO] | 4.00e+04       286  |    -5.05    2.47  200000  |     0.04      0.02
2023-10-03 19:26:42,276 [INFO] 
| `step`: Number of samples, or total training steps, or running times of `env.step()`.
| `time`: Time spent from the start of training to this moment.
| `avgR`: Average value of cumulative rewards, which is the sum of rewards in an episode.
| `stdR`: Standard dev of cumulative rewards, which is the sum of rewards in an episode.
| `avgS`: Average of steps in an episode.
| `objC`: Objective of Critic network. Or call it loss function of critic network.
| `objA`: Objective of Actor network. It is the average Q value of the critic network.
|     step      time  |     avgR    stdR    avgS  |     objC      objA
2023-10-03 19:32:07,046 [INFO] | 2.00e+04       325  |   -39.63   18.54  300000  |     0.00      0.43
2023-10-03 19:37:40,441 [INFO] | 4.00e+04       658  |   -52.80    3.74  300000  |     0.20      0.42
2023-10-03 19:43:15,536 [INFO] | 6.00e+04       993  |   -20.71   18.08  300000  |     0.20      0.42
2023-10-03 20:21:20,891 [ERROR] packet queue is empty, aborting
2023-10-03 21:06:58,113 [INFO] 
| `step`: Number of samples, or total training steps, or running times of `env.step()`.
| `time`: Time spent from the start of training to this moment.
| `avgR`: Average value of cumulative rewards, which is the sum of rewards in an episode.
| `stdR`: Standard dev of cumulative rewards, which is the sum of rewards in an episode.
| `avgS`: Average of steps in an episode.
| `objC`: Objective of Critic network. Or call it loss function of critic network.
| `objA`: Objective of Actor network. It is the average Q value of the critic network.
|     step      time  |     avgR    stdR    avgS  |     objC      objA
2023-10-03 21:13:14,994 [INFO] | 2.00e+04       377  |    97.69    6.49  300000  |     0.00      0.43
2023-10-03 21:19:24,280 [INFO] | 4.00e+04       746  |   104.98    3.53  300000  |     0.19      0.41
2023-10-03 21:25:26,656 [INFO] | 6.00e+04      1109  |   109.95    4.25  300000  |     0.29      0.42
2023-10-03 21:31:33,948 [INFO] | 8.00e+04      1476  |   108.85    4.94  300000  |     0.24      0.43
2023-10-04 20:29:27,415 [INFO] 
| `step`: Number of samples, or total training steps, or running times of `env.step()`.
| `time`: Time spent from the start of training to this moment.
| `avgR`: Average value of cumulative rewards, which is the sum of rewards in an episode.
| `stdR`: Standard dev of cumulative rewards, which is the sum of rewards in an episode.
| `avgS`: Average of steps in an episode.
| `objC`: Objective of Critic network. Or call it loss function of critic network.
| `objA`: Objective of Actor network. It is the average Q value of the critic network.
|     step      time  |     avgR    stdR    avgS  |     objC      objA
2023-10-04 20:34:58,976 [INFO] | 2.00e+04       332  |   149.67  129.34  300000  |     0.00      0.43
2023-10-04 22:19:08,525 [INFO] 
| `step`: Number of samples, or total training steps, or running times of `env.step()`.
| `time`: Time spent from the start of training to this moment.
| `avgR`: Average value of cumulative rewards, which is the sum of rewards in an episode.
| `stdR`: Standard dev of cumulative rewards, which is the sum of rewards in an episode.
| `avgS`: Average of steps in an episode.
| `objC`: Objective of Critic network. Or call it loss function of critic network.
| `objA`: Objective of Actor network. It is the average Q value of the critic network.
|     step      time  |     avgR    stdR    avgS  |     objC      objA
2023-10-04 22:24:40,871 [INFO] | 2.00e+04       332  |    17.91   28.87  300000  |     0.00      0.43
2023-10-04 22:30:21,118 [INFO] | 4.00e+04       673  |     6.20   14.02  300000  |     0.14      0.41
2023-10-04 22:35:59,245 [INFO] | 6.00e+04      1011  |   -10.20    3.50  300000  |     0.05      0.43
2023-10-04 22:41:42,650 [INFO] | 8.00e+04      1354  |   -11.20    2.25  300000  |     0.00      0.42
2023-10-04 22:47:26,624 [INFO] | 1.00e+05      1698  |   -13.88    2.69  300000  |     0.00      0.42
2023-10-04 23:32:25,777 [INFO] 
| `step`: Number of samples, or total training steps, or running times of `env.step()`.
| `time`: Time spent from the start of training to this moment.
| `avgR`: Average value of cumulative rewards, which is the sum of rewards in an episode.
| `stdR`: Standard dev of cumulative rewards, which is the sum of rewards in an episode.
| `avgS`: Average of steps in an episode.
| `objC`: Objective of Critic network. Or call it loss function of critic network.
| `objA`: Objective of Actor network. It is the average Q value of the critic network.
|     step      time  |     avgR    stdR    avgS  |     objC      objA
2023-10-04 23:34:16,774 [INFO] | 2.00e+04       111  |    42.72   19.30  100000  |     0.03      0.43
2023-10-04 23:36:06,079 [INFO] | 4.00e+04       220  |    85.41   15.54  100000  |     0.00      0.43
2023-10-04 23:37:53,864 [INFO] | 6.00e+04       328  |   102.15    7.21  100000  |     0.00      0.44
2023-10-04 23:39:41,597 [INFO] | 8.00e+04       436  |    89.14   12.61  100000  |     0.00      0.43
2023-10-04 23:41:27,821 [INFO] | 1.00e+05       542  |    96.84   12.82  100000  |     0.00      0.42
2023-10-05 00:28:54,797 [INFO] 
| `step`: Number of samples, or total training steps, or running times of `env.step()`.
| `time`: Time spent from the start of training to this moment.
| `avgR`: Average value of cumulative rewards, which is the sum of rewards in an episode.
| `stdR`: Standard dev of cumulative rewards, which is the sum of rewards in an episode.
| `avgS`: Average of steps in an episode.
| `objC`: Objective of Critic network. Or call it loss function of critic network.
| `objA`: Objective of Actor network. It is the average Q value of the critic network.
|     step      time  |     avgR    stdR    avgS  |     objC      objA
2023-10-05 00:31:03,374 [INFO] | 2.00e+04       129  |   132.82    7.38  100000  |     0.00      0.44
2023-10-05 00:33:09,257 [INFO] | 4.00e+04       254  |   155.83    7.74  100000  |     0.00      0.44
2023-10-05 00:35:16,576 [INFO] | 6.00e+04       382  |   157.86    4.87  100000  |     0.00      0.42
2023-10-05 00:37:17,369 [INFO] | 8.00e+04       503  |   147.77    4.74  100000  |     0.00      0.44
2023-10-05 00:39:18,968 [INFO] | 1.00e+05       624  |   152.60    4.09  100000  |     0.00      0.44
2023-10-05 01:24:06,350 [INFO] 
| `step`: Number of samples, or total training steps, or running times of `env.step()`.
| `time`: Time spent from the start of training to this moment.
| `avgR`: Average value of cumulative rewards, which is the sum of rewards in an episode.
| `stdR`: Standard dev of cumulative rewards, which is the sum of rewards in an episode.
| `avgS`: Average of steps in an episode.
| `objC`: Objective of Critic network. Or call it loss function of critic network.
| `objA`: Objective of Actor network. It is the average Q value of the critic network.
|     step      time  |     avgR    stdR    avgS  |     objC      objA
2023-10-05 01:31:29,068 [INFO] | 2.00e+04       443  |    96.24    6.94  300000  |     0.00      0.42
2023-10-05 01:39:02,328 [INFO] | 4.00e+04       896  |   107.71    4.47  300000  |     0.15      0.43
2023-10-05 01:47:46,859 [INFO] | 6.00e+04      1421  |   115.95    3.26  300000  |     0.53      0.43
2023-10-05 01:55:43,110 [INFO] | 8.00e+04      1897  |    99.43   11.62  300000  |     0.13      0.43
2023-10-05 02:03:18,319 [INFO] | 1.00e+05      2352  |   118.81   17.40  300000  |     0.13      0.44
2023-10-05 02:49:48,932 [INFO] 
| `step`: Number of samples, or total training steps, or running times of `env.step()`.
| `time`: Time spent from the start of training to this moment.
| `avgR`: Average value of cumulative rewards, which is the sum of rewards in an episode.
| `stdR`: Standard dev of cumulative rewards, which is the sum of rewards in an episode.
| `avgS`: Average of steps in an episode.
| `objC`: Objective of Critic network. Or call it loss function of critic network.
| `objA`: Objective of Actor network. It is the average Q value of the critic network.
|     step      time  |     avgR    stdR    avgS  |     objC      objA
2023-10-05 02:53:30,396 [INFO] | 2.00e+04       221  |    -2.87    3.88  200000  |     0.00      0.42
2023-10-05 02:57:13,062 [INFO] | 4.00e+04       444  |     0.33    4.61  200000  |     0.10      0.43
2023-10-05 03:00:58,480 [INFO] | 6.00e+04       670  |    -4.13    3.15  200000  |     0.09      0.42
2023-10-05 03:04:41,671 [INFO] | 8.00e+04       893  |     2.83   11.11  200000  |     0.00      0.43
2023-10-05 03:08:24,492 [INFO] | 1.00e+05      1116  |    -6.20    3.15  200000  |     0.00      0.43
2023-10-05 03:55:17,533 [INFO] 
| `step`: Number of samples, or total training steps, or running times of `env.step()`.
| `time`: Time spent from the start of training to this moment.
| `avgR`: Average value of cumulative rewards, which is the sum of rewards in an episode.
| `stdR`: Standard dev of cumulative rewards, which is the sum of rewards in an episode.
| `avgS`: Average of steps in an episode.
| `objC`: Objective of Critic network. Or call it loss function of critic network.
| `objA`: Objective of Actor network. It is the average Q value of the critic network.
|     step      time  |     avgR    stdR    avgS  |     objC      objA
2023-10-05 03:57:09,451 [INFO] | 2.00e+04       112  |    40.20    1.38  100000  |     0.00      0.43
2023-10-05 03:59:06,535 [INFO] | 4.00e+04       229  |    35.80    2.56  100000  |     0.00      0.42
2023-10-05 04:01:01,549 [INFO] | 6.00e+04       344  |    36.21    2.34  100000  |     0.00      0.44
2023-10-05 04:02:54,943 [INFO] | 8.00e+04       457  |    40.47    4.99  100000  |     0.00      0.43
2023-10-05 04:04:50,383 [INFO] | 1.00e+05       573  |    42.08    2.18  100000  |     0.00      0.43
2023-10-05 04:27:52,946 [ERROR] trainer1/trainer1.py Failed to reconnect: Already connected
Traceback (most recent call last):
  File "/Users/miguellopes/Documents/pod-trader/trainer1/trainer1.py", line 103, in disconnect
    sio.connect('http://localhost:5678')
  File "/Users/miguellopes/miniforge3/envs/trader/lib/python3.10/site-packages/socketio/client.py", line 306, in connect
    raise exceptions.ConnectionError('Already connected')
socketio.exceptions.ConnectionError: Already connected
2023-10-05 04:50:44,420 [INFO] 
| `step`: Number of samples, or total training steps, or running times of `env.step()`.
| `time`: Time spent from the start of training to this moment.
| `avgR`: Average value of cumulative rewards, which is the sum of rewards in an episode.
| `stdR`: Standard dev of cumulative rewards, which is the sum of rewards in an episode.
| `avgS`: Average of steps in an episode.
| `objC`: Objective of Critic network. Or call it loss function of critic network.
| `objA`: Objective of Actor network. It is the average Q value of the critic network.
|     step      time  |     avgR    stdR    avgS  |     objC      objA
2023-10-05 04:56:25,184 [INFO] | 2.00e+04       341  |    88.91    7.40  300000  |     0.00      0.43
2023-10-05 05:01:57,815 [INFO] | 4.00e+04       673  |     4.58    5.23  300000  |     0.02      0.44
2023-10-05 05:07:47,034 [INFO] | 6.00e+04      1023  |    12.48    5.51  300000  |     0.00      0.43
2023-10-05 05:13:37,468 [INFO] | 8.00e+04      1373  |    16.86    5.21  300000  |     0.00      0.42
2023-10-05 05:19:23,427 [INFO] | 1.00e+05      1719  |    12.53    4.82  300000  |     0.00      0.43
2023-10-05 06:05:51,824 [INFO] 
| `step`: Number of samples, or total training steps, or running times of `env.step()`.
| `time`: Time spent from the start of training to this moment.
| `avgR`: Average value of cumulative rewards, which is the sum of rewards in an episode.
| `stdR`: Standard dev of cumulative rewards, which is the sum of rewards in an episode.
| `avgS`: Average of steps in an episode.
| `objC`: Objective of Critic network. Or call it loss function of critic network.
| `objA`: Objective of Actor network. It is the average Q value of the critic network.
|     step      time  |     avgR    stdR    avgS  |     objC      objA
2023-10-05 06:07:34,404 [INFO] | 2.00e+04       103  |    70.56   15.37  100000  |     0.03      0.43
2023-10-05 06:09:18,072 [INFO] | 4.00e+04       206  |    73.00   19.57  100000  |     0.00      0.43
2023-10-05 06:10:58,595 [INFO] | 6.00e+04       307  |    54.53   28.78  100000  |     0.00      0.42
2023-10-05 06:12:40,054 [INFO] | 8.00e+04       408  |    76.66   15.56  100000  |     0.00      0.43
2023-10-05 06:14:22,598 [INFO] | 1.00e+05       511  |    91.20   57.11  100000  |     0.00      0.42
2023-10-05 07:02:38,133 [INFO] 
| `step`: Number of samples, or total training steps, or running times of `env.step()`.
| `time`: Time spent from the start of training to this moment.
| `avgR`: Average value of cumulative rewards, which is the sum of rewards in an episode.
| `stdR`: Standard dev of cumulative rewards, which is the sum of rewards in an episode.
| `avgS`: Average of steps in an episode.
| `objC`: Objective of Critic network. Or call it loss function of critic network.
| `objA`: Objective of Actor network. It is the average Q value of the critic network.
|     step      time  |     avgR    stdR    avgS  |     objC      objA
2023-10-05 07:08:06,530 [INFO] | 2.00e+04       328  |   100.71   16.39  300000  |     0.00      0.42
2023-10-05 07:13:34,732 [INFO] | 4.00e+04       657  |   100.65   34.51  300000  |     0.00      0.44
2023-10-05 07:19:09,030 [INFO] | 6.00e+04       991  |    93.69   23.09  300000  |     0.00      0.43
2023-10-05 07:24:35,491 [INFO] | 8.00e+04      1317  |    94.22   13.63  300000  |     0.00      0.43
2023-10-05 07:30:01,917 [INFO] | 1.00e+05      1644  |   101.50    9.82  300000  |     0.00      0.43
2023-10-05 09:14:16,596 [INFO] 
| `step`: Number of samples, or total training steps, or running times of `env.step()`.
| `time`: Time spent from the start of training to this moment.
| `avgR`: Average value of cumulative rewards, which is the sum of rewards in an episode.
| `stdR`: Standard dev of cumulative rewards, which is the sum of rewards in an episode.
| `avgS`: Average of steps in an episode.
| `objC`: Objective of Critic network. Or call it loss function of critic network.
| `objA`: Objective of Actor network. It is the average Q value of the critic network.
|     step      time  |     avgR    stdR    avgS  |     objC      objA
2023-10-05 09:19:51,939 [INFO] | 2.00e+04       335  |    69.49   22.79  300000  |     0.00      0.43
2023-10-05 10:16:28,699 [INFO] 
| `step`: Number of samples, or total training steps, or running times of `env.step()`.
| `time`: Time spent from the start of training to this moment.
| `avgR`: Average value of cumulative rewards, which is the sum of rewards in an episode.
| `stdR`: Standard dev of cumulative rewards, which is the sum of rewards in an episode.
| `avgS`: Average of steps in an episode.
| `objC`: Objective of Critic network. Or call it loss function of critic network.
| `objA`: Objective of Actor network. It is the average Q value of the critic network.
|     step      time  |     avgR    stdR    avgS  |     objC      objA
2023-10-05 10:22:07,462 [INFO] | 2.00e+04       339  |   170.11   32.47  300000  |     0.00      0.44
2023-10-05 10:27:44,912 [INFO] | 4.00e+04       676  |   180.85   23.56  300000  |     0.00      0.43
2023-10-05 10:33:33,246 [INFO] | 6.00e+04      1025  |   182.99   10.62  300000  |     0.00      0.43
2023-10-05 10:39:22,359 [INFO] | 8.00e+04      1374  |   194.73   14.65  300000  |     0.00      0.43
2023-10-05 10:45:12,686 [INFO] | 1.00e+05      1724  |   185.24    6.92  300000  |     0.00      0.43
2023-10-05 10:53:54,568 [INFO] price_array: 72540
2023-10-05 10:53:54,570 [INFO] | load actor from: ./trained_models/trainer1-232db-steps-300000[128, 64]-2019-01-01/actor.pth
2023-10-05 10:54:04,430 [INFO] Test Finished!
2023-10-05 10:54:04,430 [INFO] episode_return: 1.1512305685576112
2023-10-05 13:08:58,009 [INFO] 
| `step`: Number of samples, or total training steps, or running times of `env.step()`.
| `time`: Time spent from the start of training to this moment.
| `avgR`: Average value of cumulative rewards, which is the sum of rewards in an episode.
| `stdR`: Standard dev of cumulative rewards, which is the sum of rewards in an episode.
| `avgS`: Average of steps in an episode.
| `objC`: Objective of Critic network. Or call it loss function of critic network.
| `objA`: Objective of Actor network. It is the average Q value of the critic network.
|     step      time  |     avgR    stdR    avgS  |     objC      objA
2023-10-05 13:14:55,818 [INFO] | 2.00e+04       358  |   265.43   18.40  300000  |     0.00      0.43
2023-10-05 13:21:00,038 [INFO] | 4.00e+04       722  |   268.60   21.31  300000  |     0.00      0.43
2023-10-05 13:27:06,709 [INFO] | 6.00e+04      1089  |   276.31   16.53  300000  |     0.00      0.42
2023-10-05 13:33:08,574 [INFO] | 8.00e+04      1451  |   253.69   20.45  300000  |     0.00      0.43
2023-10-05 13:39:15,870 [INFO] | 1.00e+05      1818  |   233.81   29.11  300000  |     0.00      0.44
2023-10-05 13:48:08,294 [INFO] price_array: 72540
2023-10-05 13:48:08,296 [INFO] | load actor from: ./trained_models/trainer1-0ed0c-steps-300000[128, 64]-2019-01-01/actor.pth
2023-10-05 13:48:19,259 [INFO] Test Finished!
2023-10-05 13:48:19,260 [INFO] episode_return: 1.1690628562706995
2023-10-05 14:38:51,629 [INFO] 
| `step`: Number of samples, or total training steps, or running times of `env.step()`.
| `time`: Time spent from the start of training to this moment.
| `avgR`: Average value of cumulative rewards, which is the sum of rewards in an episode.
| `stdR`: Standard dev of cumulative rewards, which is the sum of rewards in an episode.
| `avgS`: Average of steps in an episode.
| `objC`: Objective of Critic network. Or call it loss function of critic network.
| `objA`: Objective of Actor network. It is the average Q value of the critic network.
|     step      time  |     avgR    stdR    avgS  |     objC      objA
2023-10-05 14:42:38,639 [INFO] | 2.00e+04       227  |   -17.87   14.71  200000  |     0.00      0.43
2023-10-05 14:46:31,377 [INFO] | 4.00e+04       460  |   -17.02   20.67  200000  |     0.00      0.43
2023-10-05 14:50:19,065 [INFO] | 6.00e+04       687  |   -22.09   10.42  200000  |     0.00      0.41
2023-10-05 14:54:18,625 [INFO] | 8.00e+04       927  |   -12.50    8.28  200000  |     0.00      0.41
2023-10-05 14:58:10,069 [INFO] | 1.00e+05      1158  |   -21.61   10.11  200000  |     0.00      0.43
2023-10-05 15:07:29,062 [INFO] price_array: 72540
2023-10-05 15:07:29,064 [INFO] | load actor from: ./trained_models/trainer1-5e0f2-steps-200000[128, 64, 32]-2019-01-01/actor.pth
2023-10-05 15:07:38,924 [INFO] Test Finished!
2023-10-05 15:07:38,924 [INFO] episode_return: 1.2114853312947274
2023-10-05 15:58:14,810 [INFO] 
| `step`: Number of samples, or total training steps, or running times of `env.step()`.
| `time`: Time spent from the start of training to this moment.
| `avgR`: Average value of cumulative rewards, which is the sum of rewards in an episode.
| `stdR`: Standard dev of cumulative rewards, which is the sum of rewards in an episode.
| `avgS`: Average of steps in an episode.
| `objC`: Objective of Critic network. Or call it loss function of critic network.
| `objA`: Objective of Actor network. It is the average Q value of the critic network.
|     step      time  |     avgR    stdR    avgS  |     objC      objA
2023-10-05 16:02:35,846 [INFO] | 2.00e+04       261  |     2.52    1.90  200000  |     0.00      0.43
2023-10-05 16:06:54,083 [INFO] | 4.00e+04       519  |     4.09    3.76  200000  |     0.00      0.44
2023-10-05 16:11:15,395 [INFO] | 6.00e+04       781  |     1.66    1.61  200000  |     0.00      0.43
2023-10-05 16:15:43,092 [INFO] | 8.00e+04      1048  |     4.59    1.69  200000  |     0.00      0.43
2023-10-05 16:20:16,142 [INFO] | 1.00e+05      1321  |     2.85    1.27  200000  |     0.00      0.44
2023-10-05 16:29:34,899 [INFO] price_array: 72540
2023-10-05 16:29:34,902 [INFO] | load actor from: ./trained_models/trainer1-25eb0-steps-200000[256, 128]-2019-01-01/actor.pth
2023-10-05 16:29:43,943 [INFO] Test Finished!
2023-10-05 16:29:43,944 [INFO] episode_return: 0.889415924874354
2023-10-05 17:20:19,358 [INFO] 
| `step`: Number of samples, or total training steps, or running times of `env.step()`.
| `time`: Time spent from the start of training to this moment.
| `avgR`: Average value of cumulative rewards, which is the sum of rewards in an episode.
| `stdR`: Standard dev of cumulative rewards, which is the sum of rewards in an episode.
| `avgS`: Average of steps in an episode.
| `objC`: Objective of Critic network. Or call it loss function of critic network.
| `objA`: Objective of Actor network. It is the average Q value of the critic network.
|     step      time  |     avgR    stdR    avgS  |     objC      objA
2023-10-05 17:22:17,390 [INFO] | 2.00e+04       118  |     3.29    2.67  100000  |     0.00      0.44
2023-10-05 17:24:16,799 [INFO] | 4.00e+04       237  |     1.36    1.41  100000  |     0.00      0.43
2023-10-05 17:26:24,821 [INFO] | 6.00e+04       365  |     0.40    0.44  100000  |     0.00      0.43
2023-10-05 17:28:35,921 [INFO] | 8.00e+04       497  |     0.30    0.21  100000  |     0.00      0.42
2023-10-05 17:30:48,581 [INFO] | 1.00e+05       629  |     0.52    0.40  100000  |     0.00      0.42
2023-10-05 17:41:01,396 [INFO] price_array: 72540
2023-10-05 17:41:01,399 [INFO] | load actor from: ./trained_models/trainer1-00282-steps-100000[256, 128, 64]-2019-01-01/actor.pth
2023-10-05 17:41:11,525 [INFO] Test Finished!
2023-10-05 17:41:11,526 [INFO] episode_return: 1.6557377425176645
