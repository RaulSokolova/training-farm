2023-10-03 14:15:41,567 [ERROR] packet queue is empty, aborting
2023-10-03 16:29:14,938 [ERROR] packet queue is empty, aborting
2023-10-03 17:22:59,367 [INFO] 
| `step`: Number of samples, or total training steps, or running times of `env.step()`.
| `time`: Time spent from the start of training to this moment.
| `avgR`: Average value of cumulative rewards, which is the sum of rewards in an episode.
| `stdR`: Standard dev of cumulative rewards, which is the sum of rewards in an episode.
| `avgS`: Average of steps in an episode.
| `objC`: Objective of Critic network. Or call it loss function of critic network.
| `objA`: Objective of Actor network. It is the average Q value of the critic network.
|     step      time  |     avgR    stdR    avgS  |     objC      objA
2023-10-03 17:25:42,477 [INFO] | 2.00e+04       163  |     0.00    0.00  300000  |     0.00      0.01
2023-10-03 17:28:26,050 [INFO] | 4.00e+04       327  |     0.00    0.00  300000  |     0.00      0.03
2023-10-03 17:31:13,504 [INFO] | 6.00e+04       494  |     0.00    0.00  300000  |     0.00      0.02
2023-10-03 17:33:56,592 [INFO] | 8.00e+04       657  |     0.00    0.00  300000  |     0.00      0.01
2023-10-03 17:36:39,076 [INFO] | 1.00e+05       820  |     0.00    0.00  300000  |     0.00      0.02
2023-10-03 17:39:06,324 [INFO] 
| `step`: Number of samples, or total training steps, or running times of `env.step()`.
| `time`: Time spent from the start of training to this moment.
| `avgR`: Average value of cumulative rewards, which is the sum of rewards in an episode.
| `stdR`: Standard dev of cumulative rewards, which is the sum of rewards in an episode.
| `avgS`: Average of steps in an episode.
| `objC`: Objective of Critic network. Or call it loss function of critic network.
| `objA`: Objective of Actor network. It is the average Q value of the critic network.
|     step      time  |     avgR    stdR    avgS  |     objC      objA
2023-10-03 17:41:32,863 [INFO] | 2.00e+04       147  |    -0.00    0.00  200000  |     0.00      0.01
2023-10-03 17:43:52,026 [INFO] | 4.00e+04       286  |    -5.05    2.47  200000  |     0.04      0.02
2023-10-03 19:26:42,276 [INFO] 
| `step`: Number of samples, or total training steps, or running times of `env.step()`.
| `time`: Time spent from the start of training to this moment.
| `avgR`: Average value of cumulative rewards, which is the sum of rewards in an episode.
| `stdR`: Standard dev of cumulative rewards, which is the sum of rewards in an episode.
| `avgS`: Average of steps in an episode.
| `objC`: Objective of Critic network. Or call it loss function of critic network.
| `objA`: Objective of Actor network. It is the average Q value of the critic network.
|     step      time  |     avgR    stdR    avgS  |     objC      objA
2023-10-03 19:32:07,046 [INFO] | 2.00e+04       325  |   -39.63   18.54  300000  |     0.00      0.43
2023-10-03 19:37:40,441 [INFO] | 4.00e+04       658  |   -52.80    3.74  300000  |     0.20      0.42
2023-10-03 19:43:15,536 [INFO] | 6.00e+04       993  |   -20.71   18.08  300000  |     0.20      0.42
2023-10-03 20:21:20,891 [ERROR] packet queue is empty, aborting
2023-10-03 21:06:58,113 [INFO] 
| `step`: Number of samples, or total training steps, or running times of `env.step()`.
| `time`: Time spent from the start of training to this moment.
| `avgR`: Average value of cumulative rewards, which is the sum of rewards in an episode.
| `stdR`: Standard dev of cumulative rewards, which is the sum of rewards in an episode.
| `avgS`: Average of steps in an episode.
| `objC`: Objective of Critic network. Or call it loss function of critic network.
| `objA`: Objective of Actor network. It is the average Q value of the critic network.
|     step      time  |     avgR    stdR    avgS  |     objC      objA
2023-10-03 21:13:14,994 [INFO] | 2.00e+04       377  |    97.69    6.49  300000  |     0.00      0.43
2023-10-03 21:19:24,280 [INFO] | 4.00e+04       746  |   104.98    3.53  300000  |     0.19      0.41
2023-10-03 21:25:26,656 [INFO] | 6.00e+04      1109  |   109.95    4.25  300000  |     0.29      0.42
2023-10-03 21:31:33,948 [INFO] | 8.00e+04      1476  |   108.85    4.94  300000  |     0.24      0.43
2023-10-04 20:29:27,415 [INFO] 
| `step`: Number of samples, or total training steps, or running times of `env.step()`.
| `time`: Time spent from the start of training to this moment.
| `avgR`: Average value of cumulative rewards, which is the sum of rewards in an episode.
| `stdR`: Standard dev of cumulative rewards, which is the sum of rewards in an episode.
| `avgS`: Average of steps in an episode.
| `objC`: Objective of Critic network. Or call it loss function of critic network.
| `objA`: Objective of Actor network. It is the average Q value of the critic network.
|     step      time  |     avgR    stdR    avgS  |     objC      objA
2023-10-04 20:34:58,976 [INFO] | 2.00e+04       332  |   149.67  129.34  300000  |     0.00      0.43
2023-10-04 22:19:08,525 [INFO] 
| `step`: Number of samples, or total training steps, or running times of `env.step()`.
| `time`: Time spent from the start of training to this moment.
| `avgR`: Average value of cumulative rewards, which is the sum of rewards in an episode.
| `stdR`: Standard dev of cumulative rewards, which is the sum of rewards in an episode.
| `avgS`: Average of steps in an episode.
| `objC`: Objective of Critic network. Or call it loss function of critic network.
| `objA`: Objective of Actor network. It is the average Q value of the critic network.
|     step      time  |     avgR    stdR    avgS  |     objC      objA
2023-10-04 22:24:40,871 [INFO] | 2.00e+04       332  |    17.91   28.87  300000  |     0.00      0.43
2023-10-04 22:30:21,118 [INFO] | 4.00e+04       673  |     6.20   14.02  300000  |     0.14      0.41
2023-10-04 22:35:59,245 [INFO] | 6.00e+04      1011  |   -10.20    3.50  300000  |     0.05      0.43
2023-10-04 22:41:42,650 [INFO] | 8.00e+04      1354  |   -11.20    2.25  300000  |     0.00      0.42
2023-10-04 22:47:26,624 [INFO] | 1.00e+05      1698  |   -13.88    2.69  300000  |     0.00      0.42
2023-10-04 23:32:25,777 [INFO] 
| `step`: Number of samples, or total training steps, or running times of `env.step()`.
| `time`: Time spent from the start of training to this moment.
| `avgR`: Average value of cumulative rewards, which is the sum of rewards in an episode.
| `stdR`: Standard dev of cumulative rewards, which is the sum of rewards in an episode.
| `avgS`: Average of steps in an episode.
| `objC`: Objective of Critic network. Or call it loss function of critic network.
| `objA`: Objective of Actor network. It is the average Q value of the critic network.
|     step      time  |     avgR    stdR    avgS  |     objC      objA
2023-10-04 23:34:16,774 [INFO] | 2.00e+04       111  |    42.72   19.30  100000  |     0.03      0.43
2023-10-04 23:36:06,079 [INFO] | 4.00e+04       220  |    85.41   15.54  100000  |     0.00      0.43
2023-10-04 23:37:53,864 [INFO] | 6.00e+04       328  |   102.15    7.21  100000  |     0.00      0.44
2023-10-04 23:39:41,597 [INFO] | 8.00e+04       436  |    89.14   12.61  100000  |     0.00      0.43
2023-10-04 23:41:27,821 [INFO] | 1.00e+05       542  |    96.84   12.82  100000  |     0.00      0.42
2023-10-05 00:28:54,797 [INFO] 
| `step`: Number of samples, or total training steps, or running times of `env.step()`.
| `time`: Time spent from the start of training to this moment.
| `avgR`: Average value of cumulative rewards, which is the sum of rewards in an episode.
| `stdR`: Standard dev of cumulative rewards, which is the sum of rewards in an episode.
| `avgS`: Average of steps in an episode.
| `objC`: Objective of Critic network. Or call it loss function of critic network.
| `objA`: Objective of Actor network. It is the average Q value of the critic network.
|     step      time  |     avgR    stdR    avgS  |     objC      objA
2023-10-05 00:31:03,374 [INFO] | 2.00e+04       129  |   132.82    7.38  100000  |     0.00      0.44
2023-10-05 00:33:09,257 [INFO] | 4.00e+04       254  |   155.83    7.74  100000  |     0.00      0.44
2023-10-05 00:35:16,576 [INFO] | 6.00e+04       382  |   157.86    4.87  100000  |     0.00      0.42
2023-10-05 00:37:17,369 [INFO] | 8.00e+04       503  |   147.77    4.74  100000  |     0.00      0.44
2023-10-05 00:39:18,968 [INFO] | 1.00e+05       624  |   152.60    4.09  100000  |     0.00      0.44
2023-10-05 01:24:06,350 [INFO] 
| `step`: Number of samples, or total training steps, or running times of `env.step()`.
| `time`: Time spent from the start of training to this moment.
| `avgR`: Average value of cumulative rewards, which is the sum of rewards in an episode.
| `stdR`: Standard dev of cumulative rewards, which is the sum of rewards in an episode.
| `avgS`: Average of steps in an episode.
| `objC`: Objective of Critic network. Or call it loss function of critic network.
| `objA`: Objective of Actor network. It is the average Q value of the critic network.
|     step      time  |     avgR    stdR    avgS  |     objC      objA
2023-10-05 01:31:29,068 [INFO] | 2.00e+04       443  |    96.24    6.94  300000  |     0.00      0.42
2023-10-05 01:39:02,328 [INFO] | 4.00e+04       896  |   107.71    4.47  300000  |     0.15      0.43
2023-10-05 01:47:46,859 [INFO] | 6.00e+04      1421  |   115.95    3.26  300000  |     0.53      0.43
2023-10-05 01:55:43,110 [INFO] | 8.00e+04      1897  |    99.43   11.62  300000  |     0.13      0.43
2023-10-05 02:03:18,319 [INFO] | 1.00e+05      2352  |   118.81   17.40  300000  |     0.13      0.44
2023-10-05 02:49:48,932 [INFO] 
| `step`: Number of samples, or total training steps, or running times of `env.step()`.
| `time`: Time spent from the start of training to this moment.
| `avgR`: Average value of cumulative rewards, which is the sum of rewards in an episode.
| `stdR`: Standard dev of cumulative rewards, which is the sum of rewards in an episode.
| `avgS`: Average of steps in an episode.
| `objC`: Objective of Critic network. Or call it loss function of critic network.
| `objA`: Objective of Actor network. It is the average Q value of the critic network.
|     step      time  |     avgR    stdR    avgS  |     objC      objA
2023-10-05 02:53:30,396 [INFO] | 2.00e+04       221  |    -2.87    3.88  200000  |     0.00      0.42
2023-10-05 02:57:13,062 [INFO] | 4.00e+04       444  |     0.33    4.61  200000  |     0.10      0.43
2023-10-05 03:00:58,480 [INFO] | 6.00e+04       670  |    -4.13    3.15  200000  |     0.09      0.42
2023-10-05 03:04:41,671 [INFO] | 8.00e+04       893  |     2.83   11.11  200000  |     0.00      0.43
2023-10-05 03:08:24,492 [INFO] | 1.00e+05      1116  |    -6.20    3.15  200000  |     0.00      0.43
2023-10-05 03:55:17,533 [INFO] 
| `step`: Number of samples, or total training steps, or running times of `env.step()`.
| `time`: Time spent from the start of training to this moment.
| `avgR`: Average value of cumulative rewards, which is the sum of rewards in an episode.
| `stdR`: Standard dev of cumulative rewards, which is the sum of rewards in an episode.
| `avgS`: Average of steps in an episode.
| `objC`: Objective of Critic network. Or call it loss function of critic network.
| `objA`: Objective of Actor network. It is the average Q value of the critic network.
|     step      time  |     avgR    stdR    avgS  |     objC      objA
2023-10-05 03:57:09,451 [INFO] | 2.00e+04       112  |    40.20    1.38  100000  |     0.00      0.43
2023-10-05 03:59:06,535 [INFO] | 4.00e+04       229  |    35.80    2.56  100000  |     0.00      0.42
2023-10-05 04:01:01,549 [INFO] | 6.00e+04       344  |    36.21    2.34  100000  |     0.00      0.44
2023-10-05 04:02:54,943 [INFO] | 8.00e+04       457  |    40.47    4.99  100000  |     0.00      0.43
2023-10-05 04:04:50,383 [INFO] | 1.00e+05       573  |    42.08    2.18  100000  |     0.00      0.43
2023-10-05 04:27:52,946 [ERROR] trainer1/trainer1.py Failed to reconnect: Already connected
Traceback (most recent call last):
  File "/Users/miguellopes/Documents/pod-trader/trainer1/trainer1.py", line 103, in disconnect
    sio.connect('http://localhost:5678')
  File "/Users/miguellopes/miniforge3/envs/trader/lib/python3.10/site-packages/socketio/client.py", line 306, in connect
    raise exceptions.ConnectionError('Already connected')
socketio.exceptions.ConnectionError: Already connected
2023-10-05 04:50:44,420 [INFO] 
| `step`: Number of samples, or total training steps, or running times of `env.step()`.
| `time`: Time spent from the start of training to this moment.
| `avgR`: Average value of cumulative rewards, which is the sum of rewards in an episode.
| `stdR`: Standard dev of cumulative rewards, which is the sum of rewards in an episode.
| `avgS`: Average of steps in an episode.
| `objC`: Objective of Critic network. Or call it loss function of critic network.
| `objA`: Objective of Actor network. It is the average Q value of the critic network.
|     step      time  |     avgR    stdR    avgS  |     objC      objA
2023-10-05 04:56:25,184 [INFO] | 2.00e+04       341  |    88.91    7.40  300000  |     0.00      0.43
2023-10-05 05:01:57,815 [INFO] | 4.00e+04       673  |     4.58    5.23  300000  |     0.02      0.44
2023-10-05 05:07:47,034 [INFO] | 6.00e+04      1023  |    12.48    5.51  300000  |     0.00      0.43
2023-10-05 05:13:37,468 [INFO] | 8.00e+04      1373  |    16.86    5.21  300000  |     0.00      0.42
2023-10-05 05:19:23,427 [INFO] | 1.00e+05      1719  |    12.53    4.82  300000  |     0.00      0.43
2023-10-05 06:05:51,824 [INFO] 
| `step`: Number of samples, or total training steps, or running times of `env.step()`.
| `time`: Time spent from the start of training to this moment.
| `avgR`: Average value of cumulative rewards, which is the sum of rewards in an episode.
| `stdR`: Standard dev of cumulative rewards, which is the sum of rewards in an episode.
| `avgS`: Average of steps in an episode.
| `objC`: Objective of Critic network. Or call it loss function of critic network.
| `objA`: Objective of Actor network. It is the average Q value of the critic network.
|     step      time  |     avgR    stdR    avgS  |     objC      objA
2023-10-05 06:07:34,404 [INFO] | 2.00e+04       103  |    70.56   15.37  100000  |     0.03      0.43
2023-10-05 06:09:18,072 [INFO] | 4.00e+04       206  |    73.00   19.57  100000  |     0.00      0.43
2023-10-05 06:10:58,595 [INFO] | 6.00e+04       307  |    54.53   28.78  100000  |     0.00      0.42
2023-10-05 06:12:40,054 [INFO] | 8.00e+04       408  |    76.66   15.56  100000  |     0.00      0.43
2023-10-05 06:14:22,598 [INFO] | 1.00e+05       511  |    91.20   57.11  100000  |     0.00      0.42
2023-10-05 07:02:38,133 [INFO] 
| `step`: Number of samples, or total training steps, or running times of `env.step()`.
| `time`: Time spent from the start of training to this moment.
| `avgR`: Average value of cumulative rewards, which is the sum of rewards in an episode.
| `stdR`: Standard dev of cumulative rewards, which is the sum of rewards in an episode.
| `avgS`: Average of steps in an episode.
| `objC`: Objective of Critic network. Or call it loss function of critic network.
| `objA`: Objective of Actor network. It is the average Q value of the critic network.
|     step      time  |     avgR    stdR    avgS  |     objC      objA
2023-10-05 07:08:06,530 [INFO] | 2.00e+04       328  |   100.71   16.39  300000  |     0.00      0.42
2023-10-05 07:13:34,732 [INFO] | 4.00e+04       657  |   100.65   34.51  300000  |     0.00      0.44
2023-10-05 07:19:09,030 [INFO] | 6.00e+04       991  |    93.69   23.09  300000  |     0.00      0.43
2023-10-05 07:24:35,491 [INFO] | 8.00e+04      1317  |    94.22   13.63  300000  |     0.00      0.43
2023-10-05 07:30:01,917 [INFO] | 1.00e+05      1644  |   101.50    9.82  300000  |     0.00      0.43
2023-10-05 09:14:16,596 [INFO] 
| `step`: Number of samples, or total training steps, or running times of `env.step()`.
| `time`: Time spent from the start of training to this moment.
| `avgR`: Average value of cumulative rewards, which is the sum of rewards in an episode.
| `stdR`: Standard dev of cumulative rewards, which is the sum of rewards in an episode.
| `avgS`: Average of steps in an episode.
| `objC`: Objective of Critic network. Or call it loss function of critic network.
| `objA`: Objective of Actor network. It is the average Q value of the critic network.
|     step      time  |     avgR    stdR    avgS  |     objC      objA
2023-10-05 09:19:51,939 [INFO] | 2.00e+04       335  |    69.49   22.79  300000  |     0.00      0.43
2023-10-05 10:16:28,699 [INFO] 
| `step`: Number of samples, or total training steps, or running times of `env.step()`.
| `time`: Time spent from the start of training to this moment.
| `avgR`: Average value of cumulative rewards, which is the sum of rewards in an episode.
| `stdR`: Standard dev of cumulative rewards, which is the sum of rewards in an episode.
| `avgS`: Average of steps in an episode.
| `objC`: Objective of Critic network. Or call it loss function of critic network.
| `objA`: Objective of Actor network. It is the average Q value of the critic network.
|     step      time  |     avgR    stdR    avgS  |     objC      objA
2023-10-05 10:22:07,462 [INFO] | 2.00e+04       339  |   170.11   32.47  300000  |     0.00      0.44
2023-10-05 10:27:44,912 [INFO] | 4.00e+04       676  |   180.85   23.56  300000  |     0.00      0.43
2023-10-05 10:33:33,246 [INFO] | 6.00e+04      1025  |   182.99   10.62  300000  |     0.00      0.43
2023-10-05 10:39:22,359 [INFO] | 8.00e+04      1374  |   194.73   14.65  300000  |     0.00      0.43
2023-10-05 10:45:12,686 [INFO] | 1.00e+05      1724  |   185.24    6.92  300000  |     0.00      0.43
2023-10-05 10:53:54,568 [INFO] price_array: 72540
2023-10-05 10:53:54,570 [INFO] | load actor from: ./trained_models/trainer1-232db-steps-300000[128, 64]-2019-01-01/actor.pth
2023-10-05 10:54:04,430 [INFO] Test Finished!
2023-10-05 10:54:04,430 [INFO] episode_return: 1.1512305685576112
2023-10-05 13:08:58,009 [INFO] 
| `step`: Number of samples, or total training steps, or running times of `env.step()`.
| `time`: Time spent from the start of training to this moment.
| `avgR`: Average value of cumulative rewards, which is the sum of rewards in an episode.
| `stdR`: Standard dev of cumulative rewards, which is the sum of rewards in an episode.
| `avgS`: Average of steps in an episode.
| `objC`: Objective of Critic network. Or call it loss function of critic network.
| `objA`: Objective of Actor network. It is the average Q value of the critic network.
|     step      time  |     avgR    stdR    avgS  |     objC      objA
2023-10-05 13:14:55,818 [INFO] | 2.00e+04       358  |   265.43   18.40  300000  |     0.00      0.43
2023-10-05 13:21:00,038 [INFO] | 4.00e+04       722  |   268.60   21.31  300000  |     0.00      0.43
2023-10-05 13:27:06,709 [INFO] | 6.00e+04      1089  |   276.31   16.53  300000  |     0.00      0.42
2023-10-05 13:33:08,574 [INFO] | 8.00e+04      1451  |   253.69   20.45  300000  |     0.00      0.43
2023-10-05 13:39:15,870 [INFO] | 1.00e+05      1818  |   233.81   29.11  300000  |     0.00      0.44
2023-10-05 13:48:08,294 [INFO] price_array: 72540
2023-10-05 13:48:08,296 [INFO] | load actor from: ./trained_models/trainer1-0ed0c-steps-300000[128, 64]-2019-01-01/actor.pth
2023-10-05 13:48:19,259 [INFO] Test Finished!
2023-10-05 13:48:19,260 [INFO] episode_return: 1.1690628562706995
2023-10-05 14:38:51,629 [INFO] 
| `step`: Number of samples, or total training steps, or running times of `env.step()`.
| `time`: Time spent from the start of training to this moment.
| `avgR`: Average value of cumulative rewards, which is the sum of rewards in an episode.
| `stdR`: Standard dev of cumulative rewards, which is the sum of rewards in an episode.
| `avgS`: Average of steps in an episode.
| `objC`: Objective of Critic network. Or call it loss function of critic network.
| `objA`: Objective of Actor network. It is the average Q value of the critic network.
|     step      time  |     avgR    stdR    avgS  |     objC      objA
2023-10-05 14:42:38,639 [INFO] | 2.00e+04       227  |   -17.87   14.71  200000  |     0.00      0.43
2023-10-05 14:46:31,377 [INFO] | 4.00e+04       460  |   -17.02   20.67  200000  |     0.00      0.43
2023-10-05 14:50:19,065 [INFO] | 6.00e+04       687  |   -22.09   10.42  200000  |     0.00      0.41
2023-10-05 14:54:18,625 [INFO] | 8.00e+04       927  |   -12.50    8.28  200000  |     0.00      0.41
2023-10-05 14:58:10,069 [INFO] | 1.00e+05      1158  |   -21.61   10.11  200000  |     0.00      0.43
2023-10-05 15:07:29,062 [INFO] price_array: 72540
2023-10-05 15:07:29,064 [INFO] | load actor from: ./trained_models/trainer1-5e0f2-steps-200000[128, 64, 32]-2019-01-01/actor.pth
2023-10-05 15:07:38,924 [INFO] Test Finished!
2023-10-05 15:07:38,924 [INFO] episode_return: 1.2114853312947274
2023-10-05 15:58:14,810 [INFO] 
| `step`: Number of samples, or total training steps, or running times of `env.step()`.
| `time`: Time spent from the start of training to this moment.
| `avgR`: Average value of cumulative rewards, which is the sum of rewards in an episode.
| `stdR`: Standard dev of cumulative rewards, which is the sum of rewards in an episode.
| `avgS`: Average of steps in an episode.
| `objC`: Objective of Critic network. Or call it loss function of critic network.
| `objA`: Objective of Actor network. It is the average Q value of the critic network.
|     step      time  |     avgR    stdR    avgS  |     objC      objA
2023-10-05 16:02:35,846 [INFO] | 2.00e+04       261  |     2.52    1.90  200000  |     0.00      0.43
2023-10-05 16:06:54,083 [INFO] | 4.00e+04       519  |     4.09    3.76  200000  |     0.00      0.44
2023-10-05 16:11:15,395 [INFO] | 6.00e+04       781  |     1.66    1.61  200000  |     0.00      0.43
2023-10-05 16:15:43,092 [INFO] | 8.00e+04      1048  |     4.59    1.69  200000  |     0.00      0.43
2023-10-05 16:20:16,142 [INFO] | 1.00e+05      1321  |     2.85    1.27  200000  |     0.00      0.44
2023-10-05 16:29:34,899 [INFO] price_array: 72540
2023-10-05 16:29:34,902 [INFO] | load actor from: ./trained_models/trainer1-25eb0-steps-200000[256, 128]-2019-01-01/actor.pth
2023-10-05 16:29:43,943 [INFO] Test Finished!
2023-10-05 16:29:43,944 [INFO] episode_return: 0.889415924874354
2023-10-05 17:20:19,358 [INFO] 
| `step`: Number of samples, or total training steps, or running times of `env.step()`.
| `time`: Time spent from the start of training to this moment.
| `avgR`: Average value of cumulative rewards, which is the sum of rewards in an episode.
| `stdR`: Standard dev of cumulative rewards, which is the sum of rewards in an episode.
| `avgS`: Average of steps in an episode.
| `objC`: Objective of Critic network. Or call it loss function of critic network.
| `objA`: Objective of Actor network. It is the average Q value of the critic network.
|     step      time  |     avgR    stdR    avgS  |     objC      objA
2023-10-05 17:22:17,390 [INFO] | 2.00e+04       118  |     3.29    2.67  100000  |     0.00      0.44
2023-10-05 17:24:16,799 [INFO] | 4.00e+04       237  |     1.36    1.41  100000  |     0.00      0.43
2023-10-05 17:26:24,821 [INFO] | 6.00e+04       365  |     0.40    0.44  100000  |     0.00      0.43
2023-10-05 17:28:35,921 [INFO] | 8.00e+04       497  |     0.30    0.21  100000  |     0.00      0.42
2023-10-05 17:30:48,581 [INFO] | 1.00e+05       629  |     0.52    0.40  100000  |     0.00      0.42
2023-10-05 17:41:01,396 [INFO] price_array: 72540
2023-10-05 17:41:01,399 [INFO] | load actor from: ./trained_models/trainer1-00282-steps-100000[256, 128, 64]-2019-01-01/actor.pth
2023-10-05 17:41:11,525 [INFO] Test Finished!
2023-10-05 17:41:11,526 [INFO] episode_return: 1.6557377425176645
2023-10-05 18:31:23,868 [INFO] 
| `step`: Number of samples, or total training steps, or running times of `env.step()`.
| `time`: Time spent from the start of training to this moment.
| `avgR`: Average value of cumulative rewards, which is the sum of rewards in an episode.
| `stdR`: Standard dev of cumulative rewards, which is the sum of rewards in an episode.
| `avgS`: Average of steps in an episode.
| `objC`: Objective of Critic network. Or call it loss function of critic network.
| `objA`: Objective of Actor network. It is the average Q value of the critic network.
|     step      time  |     avgR    stdR    avgS  |     objC      objA
2023-10-05 18:35:27,393 [INFO] | 2.00e+04       244  |     7.09    1.29  200000  |     0.00      0.42
2023-10-05 18:39:33,589 [INFO] | 4.00e+04       490  |     5.46    3.10  200000  |     0.00      0.43
2023-10-05 18:43:50,591 [INFO] | 6.00e+04       747  |     7.74    7.24  200000  |     0.00      0.43
2023-10-05 18:48:08,930 [INFO] | 8.00e+04      1005  |     5.98    1.47  200000  |     0.00      0.42
2023-10-05 18:52:44,717 [INFO] | 1.00e+05      1281  |    -1.02    2.44  200000  |     0.00      0.42
2023-10-05 19:02:38,672 [INFO] price_array: 72540
2023-10-05 19:02:38,675 [INFO] | load actor from: ./trained_models/trainer1-eca0c-steps-200000[512, 256]-2019-01-01/actor.pth
2023-10-05 19:02:48,781 [INFO] Test Finished!
2023-10-05 19:02:48,782 [INFO] episode_return: 1.2166015250186217
2023-10-05 19:54:52,991 [INFO] 
| `step`: Number of samples, or total training steps, or running times of `env.step()`.
| `time`: Time spent from the start of training to this moment.
| `avgR`: Average value of cumulative rewards, which is the sum of rewards in an episode.
| `stdR`: Standard dev of cumulative rewards, which is the sum of rewards in an episode.
| `avgS`: Average of steps in an episode.
| `objC`: Objective of Critic network. Or call it loss function of critic network.
| `objA`: Objective of Actor network. It is the average Q value of the critic network.
|     step      time  |     avgR    stdR    avgS  |     objC      objA
2023-10-05 20:01:24,002 [INFO] | 2.00e+04       391  |   -29.21   16.29  300000  |     0.00      0.44
2023-10-05 20:08:00,386 [INFO] | 4.00e+04       787  |   -23.01    9.42  300000  |     0.00      0.42
2023-10-05 20:14:35,303 [INFO] | 6.00e+04      1182  |   -24.76   13.51  300000  |     0.00      0.43
2023-10-05 20:21:05,768 [INFO] | 8.00e+04      1573  |   -16.94   14.57  300000  |     0.00      0.43
2023-10-05 20:27:41,178 [INFO] | 1.00e+05      1968  |   -21.98   12.52  300000  |     0.00      0.42
2023-10-05 20:37:58,974 [INFO] price_array: 72540
2023-10-05 20:37:58,976 [INFO] | load actor from: ./trained_models/trainer1-551a9-steps-300000[128, 64]-2019-01-01/actor.pth
2023-10-05 20:38:08,775 [INFO] Test Finished!
2023-10-05 20:38:08,776 [INFO] episode_return: 1.0908120630420695
2023-10-05 21:32:22,106 [INFO] 
| `step`: Number of samples, or total training steps, or running times of `env.step()`.
| `time`: Time spent from the start of training to this moment.
| `avgR`: Average value of cumulative rewards, which is the sum of rewards in an episode.
| `stdR`: Standard dev of cumulative rewards, which is the sum of rewards in an episode.
| `avgS`: Average of steps in an episode.
| `objC`: Objective of Critic network. Or call it loss function of critic network.
| `objA`: Objective of Actor network. It is the average Q value of the critic network.
|     step      time  |     avgR    stdR    avgS  |     objC      objA
2023-10-05 21:34:23,308 [INFO] | 2.00e+04       121  |     0.05    0.15  100000  |     0.00      0.42
2023-10-05 21:36:23,323 [INFO] | 4.00e+04       241  |     0.01    0.14  100000  |     0.00      0.41
2023-10-05 21:38:24,515 [INFO] | 6.00e+04       362  |    -0.01    0.12  100000  |     0.00      0.43
2023-10-05 21:40:26,080 [INFO] | 8.00e+04       484  |     0.05    0.18  100000  |     0.00      0.42
2023-10-05 21:42:28,252 [INFO] | 1.00e+05       606  |     0.03    0.19  100000  |     0.00      0.44
2023-10-05 21:52:40,471 [INFO] price_array: 72540
2023-10-05 21:52:40,474 [INFO] | load actor from: ./trained_models/trainer1-c8c35-steps-100000[256, 128, 64]-2019-01-01/actor.pth
2023-10-05 21:52:50,237 [INFO] Test Finished!
2023-10-05 21:52:50,237 [INFO] episode_return: 0.7834488605990892
2023-10-05 22:48:12,677 [INFO] 
| `step`: Number of samples, or total training steps, or running times of `env.step()`.
| `time`: Time spent from the start of training to this moment.
| `avgR`: Average value of cumulative rewards, which is the sum of rewards in an episode.
| `stdR`: Standard dev of cumulative rewards, which is the sum of rewards in an episode.
| `avgS`: Average of steps in an episode.
| `objC`: Objective of Critic network. Or call it loss function of critic network.
| `objA`: Objective of Actor network. It is the average Q value of the critic network.
|     step      time  |     avgR    stdR    avgS  |     objC      objA
2023-10-05 22:56:27,590 [INFO] | 2.00e+04       495  |    -8.23    5.83  300000  |     0.00      0.43
2023-10-05 23:04:10,397 [INFO] | 4.00e+04       958  |   -10.31   10.21  300000  |     0.00      0.44
2023-10-05 23:12:13,292 [INFO] | 6.00e+04      1441  |    -1.33    6.31  300000  |     0.00      0.43
2023-10-05 23:19:53,946 [INFO] | 8.00e+04      1901  |    -9.85    6.63  300000  |     0.00      0.43
2023-10-05 23:27:30,184 [INFO] | 1.00e+05      2358  |   -11.19    8.31  300000  |     0.00      0.45
2023-10-05 23:35:58,989 [INFO] price_array: 72540
2023-10-05 23:35:58,997 [INFO] | load actor from: ./trained_models/trainer1-2b5eb-steps-300000[1024, 512]-2019-01-01/actor.pth
2023-10-05 23:36:10,450 [INFO] Test Finished!
2023-10-05 23:36:10,450 [INFO] episode_return: 1.2166860180665027
2023-10-06 00:24:54,097 [INFO] 
| `step`: Number of samples, or total training steps, or running times of `env.step()`.
| `time`: Time spent from the start of training to this moment.
| `avgR`: Average value of cumulative rewards, which is the sum of rewards in an episode.
| `stdR`: Standard dev of cumulative rewards, which is the sum of rewards in an episode.
| `avgS`: Average of steps in an episode.
| `objC`: Objective of Critic network. Or call it loss function of critic network.
| `objA`: Objective of Actor network. It is the average Q value of the critic network.
|     step      time  |     avgR    stdR    avgS  |     objC      objA
2023-10-06 00:27:00,941 [INFO] | 2.00e+04       127  |    -0.10    0.03  100000  |     0.00      0.45
2023-10-06 00:29:03,087 [INFO] | 4.00e+04       249  |    -0.12    0.03  100000  |     0.00      0.43
2023-10-06 00:31:04,782 [INFO] | 6.00e+04       371  |    -0.13    0.06  100000  |     0.00      0.43
2023-10-06 00:33:10,660 [INFO] | 8.00e+04       497  |    -0.11    0.05  100000  |     0.00      0.43
2023-10-06 00:35:17,139 [INFO] | 1.00e+05       623  |    -0.10    0.05  100000  |     0.00      0.44
2023-10-06 00:44:05,566 [INFO] price_array: 72540
2023-10-06 00:44:05,570 [INFO] | load actor from: ./trained_models/trainer1-33495-steps-100000[256, 128]-2019-01-01/actor.pth
2023-10-06 00:44:14,904 [INFO] Test Finished!
2023-10-06 00:44:14,905 [INFO] episode_return: 1.1162493314456967
2023-10-06 01:33:40,157 [INFO] 
| `step`: Number of samples, or total training steps, or running times of `env.step()`.
| `time`: Time spent from the start of training to this moment.
| `avgR`: Average value of cumulative rewards, which is the sum of rewards in an episode.
| `stdR`: Standard dev of cumulative rewards, which is the sum of rewards in an episode.
| `avgS`: Average of steps in an episode.
| `objC`: Objective of Critic network. Or call it loss function of critic network.
| `objA`: Objective of Actor network. It is the average Q value of the critic network.
|     step      time  |     avgR    stdR    avgS  |     objC      objA
2023-10-06 01:39:31,231 [INFO] | 2.00e+04       351  |    19.25    7.85  300000  |     0.00      0.43
2023-10-06 01:45:26,451 [INFO] | 4.00e+04       706  |    50.30   28.52  300000  |     0.00      0.43
2023-10-06 01:50:49,202 [INFO] | 6.00e+04      1029  |    59.69   22.96  300000  |     0.00      0.42
2023-10-06 01:56:03,061 [INFO] | 8.00e+04      1343  |    34.82   14.26  300000  |     0.00      0.43
2023-10-06 02:01:18,939 [INFO] | 1.00e+05      1659  |    58.99   12.90  300000  |     0.00      0.43
2023-10-06 02:09:54,075 [INFO] price_array: 72540
2023-10-06 02:09:54,078 [INFO] | load actor from: ./trained_models/trainer1-ca0c7-steps-300000[256, 128, 64]-2019-01-01/actor.pth
2023-10-06 02:10:03,023 [INFO] Test Finished!
2023-10-06 02:10:03,023 [INFO] episode_return: 0.8958020828106519
2023-10-06 02:58:49,816 [INFO] 
| `step`: Number of samples, or total training steps, or running times of `env.step()`.
| `time`: Time spent from the start of training to this moment.
| `avgR`: Average value of cumulative rewards, which is the sum of rewards in an episode.
| `stdR`: Standard dev of cumulative rewards, which is the sum of rewards in an episode.
| `avgS`: Average of steps in an episode.
| `objC`: Objective of Critic network. Or call it loss function of critic network.
| `objA`: Objective of Actor network. It is the average Q value of the critic network.
|     step      time  |     avgR    stdR    avgS  |     objC      objA
2023-10-06 03:01:00,961 [INFO] | 2.00e+04       131  |    -0.05    0.07  100000  |     0.00      0.43
2023-10-06 03:03:11,747 [INFO] | 4.00e+04       262  |    -0.00    0.11  100000  |     0.00      0.43
2023-10-06 03:05:21,123 [INFO] | 6.00e+04       391  |     0.17    0.12  100000  |     0.00      0.41
2023-10-06 03:07:33,975 [INFO] | 8.00e+04       524  |     0.17    0.11  100000  |     0.00      0.41
2023-10-06 03:09:43,773 [INFO] | 1.00e+05       654  |     0.05    0.08  100000  |     0.00      0.43
2023-10-06 03:18:29,637 [INFO] price_array: 72540
2023-10-06 03:18:29,640 [INFO] | load actor from: ./trained_models/trainer1-688eb-steps-100000[512, 256]-2019-01-01/actor.pth
2023-10-06 03:18:39,383 [INFO] Test Finished!
2023-10-06 03:18:39,383 [INFO] episode_return: 1.5674975646816853
2023-10-06 04:09:36,207 [INFO] 
| `step`: Number of samples, or total training steps, or running times of `env.step()`.
| `time`: Time spent from the start of training to this moment.
| `avgR`: Average value of cumulative rewards, which is the sum of rewards in an episode.
| `stdR`: Standard dev of cumulative rewards, which is the sum of rewards in an episode.
| `avgS`: Average of steps in an episode.
| `objC`: Objective of Critic network. Or call it loss function of critic network.
| `objA`: Objective of Actor network. It is the average Q value of the critic network.
|     step      time  |     avgR    stdR    avgS  |     objC      objA
2023-10-06 04:13:48,605 [INFO] | 2.00e+04       252  |    -0.08    4.42  200000  |     0.00      0.43
2023-10-06 04:18:03,919 [INFO] | 4.00e+04       508  |    -1.66    5.78  200000  |     0.00      0.43
2023-10-06 04:22:12,226 [INFO] | 6.00e+04       756  |    -0.94    4.63  200000  |     0.00      0.43
2023-10-06 04:26:12,799 [INFO] | 8.00e+04       997  |     0.91    5.38  200000  |     0.00      0.42
2023-10-06 04:30:11,731 [INFO] | 1.00e+05      1236  |     2.68    2.86  200000  |     0.00      0.43
2023-10-06 04:38:40,040 [INFO] price_array: 72540
2023-10-06 04:38:40,045 [INFO] | load actor from: ./trained_models/trainer1-167c2-steps-200000[512, 256, 128]-2019-01-01/actor.pth
2023-10-06 04:38:49,691 [INFO] Test Finished!
2023-10-06 04:38:49,692 [INFO] episode_return: 1.0768313414252306
2023-10-06 05:26:58,929 [INFO] 
| `step`: Number of samples, or total training steps, or running times of `env.step()`.
| `time`: Time spent from the start of training to this moment.
| `avgR`: Average value of cumulative rewards, which is the sum of rewards in an episode.
| `stdR`: Standard dev of cumulative rewards, which is the sum of rewards in an episode.
| `avgS`: Average of steps in an episode.
| `objC`: Objective of Critic network. Or call it loss function of critic network.
| `objA`: Objective of Actor network. It is the average Q value of the critic network.
|     step      time  |     avgR    stdR    avgS  |     objC      objA
2023-10-06 05:29:40,741 [INFO] | 2.00e+04       162  |    -0.08    0.07  100000  |     0.00      0.40
2023-10-06 05:32:19,379 [INFO] | 4.00e+04       320  |    -0.01    0.03  100000  |     0.00      0.42
2023-10-06 05:35:00,737 [INFO] | 6.00e+04       482  |    -0.02    0.02  100000  |     0.00      0.41
2023-10-06 05:37:43,592 [INFO] | 8.00e+04       645  |    -0.05    0.08  100000  |     0.00      0.43
2023-10-06 05:40:26,220 [INFO] | 1.00e+05       807  |    -0.02    0.02  100000  |     0.00      0.41
2023-10-06 05:48:59,757 [INFO] price_array: 72540
2023-10-06 05:48:59,765 [INFO] | load actor from: ./trained_models/trainer1-94c63-steps-100000[1024, 512]-2019-01-01/actor.pth
2023-10-06 05:49:11,046 [INFO] Test Finished!
2023-10-06 05:49:11,046 [INFO] episode_return: 1.051022542650035
2023-10-06 06:35:28,599 [INFO] 
| `step`: Number of samples, or total training steps, or running times of `env.step()`.
| `time`: Time spent from the start of training to this moment.
| `avgR`: Average value of cumulative rewards, which is the sum of rewards in an episode.
| `stdR`: Standard dev of cumulative rewards, which is the sum of rewards in an episode.
| `avgS`: Average of steps in an episode.
| `objC`: Objective of Critic network. Or call it loss function of critic network.
| `objA`: Objective of Actor network. It is the average Q value of the critic network.
|     step      time  |     avgR    stdR    avgS  |     objC      objA
2023-10-06 06:37:40,756 [INFO] | 2.00e+04       132  |    -0.13    0.06  100000  |     0.00      0.43
2023-10-06 06:40:00,001 [INFO] | 4.00e+04       271  |    -0.10    0.07  100000  |     0.00      0.43
2023-10-06 06:42:16,247 [INFO] | 6.00e+04       408  |    -0.17    0.07  100000  |     0.00      0.44
2023-10-06 06:44:35,433 [INFO] | 8.00e+04       547  |    -0.19    0.07  100000  |     0.00      0.42
2023-10-06 06:46:46,569 [INFO] | 1.00e+05       678  |    -0.16    0.09  100000  |     0.00      0.42
2023-10-06 06:55:07,266 [INFO] price_array: 72540
2023-10-06 06:55:07,269 [INFO] | load actor from: ./trained_models/trainer1-252ed-steps-100000[512, 256, 128]-2019-01-01/actor.pth
2023-10-06 06:55:17,189 [INFO] Test Finished!
2023-10-06 06:55:17,190 [INFO] episode_return: 1.593616488955164
2023-10-06 07:43:09,232 [INFO] 
| `step`: Number of samples, or total training steps, or running times of `env.step()`.
| `time`: Time spent from the start of training to this moment.
| `avgR`: Average value of cumulative rewards, which is the sum of rewards in an episode.
| `stdR`: Standard dev of cumulative rewards, which is the sum of rewards in an episode.
| `avgS`: Average of steps in an episode.
| `objC`: Objective of Critic network. Or call it loss function of critic network.
| `objA`: Objective of Actor network. It is the average Q value of the critic network.
|     step      time  |     avgR    stdR    avgS  |     objC      objA
2023-10-06 07:48:24,966 [INFO] | 2.00e+04       316  |    -3.64    4.52  200000  |     0.00      0.43
2023-10-06 07:53:51,896 [INFO] | 4.00e+04       643  |    -2.96    4.28  200000  |     0.00      0.43
2023-10-06 07:59:17,000 [INFO] | 6.00e+04       968  |     1.10    7.43  200000  |     0.00      0.43
2023-10-06 08:04:39,215 [INFO] | 8.00e+04      1290  |     4.42    5.84  200000  |     0.00      0.42
2023-10-06 08:10:02,679 [INFO] | 1.00e+05      1613  |    -3.77    5.91  200000  |     0.00      0.43
2023-10-06 08:18:34,632 [INFO] price_array: 72540
2023-10-06 08:18:34,639 [INFO] | load actor from: ./trained_models/trainer1-20405-steps-200000[1024, 512]-2019-01-01/actor.pth
2023-10-06 08:18:46,650 [INFO] Test Finished!
2023-10-06 08:18:46,651 [INFO] episode_return: 1.7807512010113717
2023-10-06 09:06:55,158 [INFO] 
| `step`: Number of samples, or total training steps, or running times of `env.step()`.
| `time`: Time spent from the start of training to this moment.
| `avgR`: Average value of cumulative rewards, which is the sum of rewards in an episode.
| `stdR`: Standard dev of cumulative rewards, which is the sum of rewards in an episode.
| `avgS`: Average of steps in an episode.
| `objC`: Objective of Critic network. Or call it loss function of critic network.
| `objA`: Objective of Actor network. It is the average Q value of the critic network.
|     step      time  |     avgR    stdR    avgS  |     objC      objA
2023-10-06 09:08:53,896 [INFO] | 2.00e+04       119  |     0.18    0.05  100000  |     0.00      0.42
2023-10-06 09:10:55,041 [INFO] | 4.00e+04       240  |     0.18    0.04  100000  |     0.00      0.42
2023-10-06 09:12:56,134 [INFO] | 6.00e+04       361  |     0.18    0.06  100000  |     0.00      0.44
2023-10-06 09:15:00,665 [INFO] | 8.00e+04       486  |     0.21    0.05  100000  |     0.00      0.43
2023-10-06 09:17:05,938 [INFO] | 1.00e+05       611  |     0.26    0.05  100000  |     0.00      0.44
2023-10-06 09:25:58,488 [INFO] price_array: 72540
2023-10-06 09:25:58,491 [INFO] | load actor from: ./trained_models/trainer1-d7250-steps-100000[128, 64]-2019-01-01/actor.pth
2023-10-06 09:26:07,806 [INFO] Test Finished!
2023-10-06 09:26:07,806 [INFO] episode_return: 1.058396319523526
2023-10-06 10:22:29,711 [INFO] 
| `step`: Number of samples, or total training steps, or running times of `env.step()`.
| `time`: Time spent from the start of training to this moment.
| `avgR`: Average value of cumulative rewards, which is the sum of rewards in an episode.
| `stdR`: Standard dev of cumulative rewards, which is the sum of rewards in an episode.
| `avgS`: Average of steps in an episode.
| `objC`: Objective of Critic network. Or call it loss function of critic network.
| `objA`: Objective of Actor network. It is the average Q value of the critic network.
|     step      time  |     avgR    stdR    avgS  |     objC      objA
2023-10-06 10:28:23,002 [INFO] | 2.00e+04       353  |    11.52    5.07  200000  |     0.00      0.42
2023-10-06 10:34:25,327 [INFO] | 4.00e+04       716  |    -3.95    4.14  200000  |     0.00      0.42
2023-10-06 10:40:43,343 [INFO] | 6.00e+04      1094  |    -5.46    7.30  200000  |     0.00      0.43
2023-10-06 10:47:04,564 [INFO] | 8.00e+04      1475  |     8.00    2.90  200000  |     0.00      0.41
2023-10-06 10:53:10,388 [INFO] | 1.00e+05      1841  |     6.79    2.74  200000  |     0.00      0.43
2023-10-06 11:03:00,926 [INFO] price_array: 72540
2023-10-06 11:03:00,934 [INFO] | load actor from: ./trained_models/trainer1-9229c-steps-200000[1024, 512]-2019-01-01/actor.pth
2023-10-06 11:03:13,075 [INFO] Test Finished!
2023-10-06 11:03:13,075 [INFO] episode_return: 1.2663047333092217
2023-10-06 11:54:53,145 [INFO] 
| `step`: Number of samples, or total training steps, or running times of `env.step()`.
| `time`: Time spent from the start of training to this moment.
| `avgR`: Average value of cumulative rewards, which is the sum of rewards in an episode.
| `stdR`: Standard dev of cumulative rewards, which is the sum of rewards in an episode.
| `avgS`: Average of steps in an episode.
| `objC`: Objective of Critic network. Or call it loss function of critic network.
| `objA`: Objective of Actor network. It is the average Q value of the critic network.
|     step      time  |     avgR    stdR    avgS  |     objC      objA
2023-10-06 11:59:42,907 [INFO] | 2.00e+04       290  |   -18.03   19.06  200000  |     0.00      0.44
2023-10-06 12:04:30,501 [INFO] | 4.00e+04       577  |   -15.52   19.08  200000  |     0.00      0.42
2023-10-06 12:09:20,711 [INFO] | 6.00e+04       868  |   -11.25   17.93  200000  |     0.00      0.43
2023-10-06 12:13:56,240 [INFO] | 8.00e+04      1143  |    -1.09    6.02  200000  |     0.00      0.43
2023-10-06 12:18:46,186 [INFO] | 1.00e+05      1433  |   -22.31   17.14  200000  |     0.00      0.41
2023-10-06 12:28:43,758 [INFO] price_array: 72540
2023-10-06 12:28:43,762 [INFO] | load actor from: ./trained_models/trainer1-2f7ff-steps-200000[512, 256]-2019-01-01/actor.pth
2023-10-06 12:28:55,749 [INFO] Test Finished!
2023-10-06 12:28:55,749 [INFO] episode_return: 0.977540812898398
2023-10-06 13:20:23,099 [INFO] 
| `step`: Number of samples, or total training steps, or running times of `env.step()`.
| `time`: Time spent from the start of training to this moment.
| `avgR`: Average value of cumulative rewards, which is the sum of rewards in an episode.
| `stdR`: Standard dev of cumulative rewards, which is the sum of rewards in an episode.
| `avgS`: Average of steps in an episode.
| `objC`: Objective of Critic network. Or call it loss function of critic network.
| `objA`: Objective of Actor network. It is the average Q value of the critic network.
|     step      time  |     avgR    stdR    avgS  |     objC      objA
2023-10-06 13:27:15,682 [INFO] | 2.00e+04       413  |    27.73    8.74  300000  |     0.00      0.43
2023-10-06 13:34:13,386 [INFO] | 4.00e+04       830  |    26.05    5.42  300000  |     0.00      0.41
2023-10-06 13:41:11,648 [INFO] | 6.00e+04      1249  |    31.15   10.06  300000  |     0.00      0.42
2023-10-06 13:47:37,965 [INFO] | 8.00e+04      1635  |    30.62    8.11  300000  |     0.00      0.43
2023-10-06 13:54:18,315 [INFO] | 1.00e+05      2035  |    31.12   10.76  300000  |     0.00      0.43
2023-10-06 14:03:10,695 [INFO] price_array: 72540
2023-10-06 14:03:10,699 [INFO] | load actor from: ./trained_models/trainer1-93708-steps-300000[512, 256, 128]-2019-01-01/actor.pth
2023-10-06 14:03:22,161 [INFO] Test Finished!
2023-10-06 14:03:22,162 [INFO] episode_return: 1.0045525843870164
